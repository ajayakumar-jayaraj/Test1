KUBERNETES NOTES:


apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - image: busybox:1.28.4
    command:
      - sleep
      - "3600"
    name: busybox
  restartPolicy: Always



  kubectl get pods -o custom-columns=POD:metadata.name,NODE:spec.nodeNAME —sort-by spec.nodeNAME -n kube-system


  #curl localhost:8001/api/v1/namespaces/my-ns/services_project_name


  # cat /var/run/secrets/kubernetes.io/sericeaccount/token

  curl --head http://127.0.0.0.1:8081   #to curl the port forward running

  curl -I localhost:<node port>   # to view headers of the deployment

  View the version of the server and client on the master node:

kubectl version --short
View the version of the scheduler and controller manager:

kubectl get pods -n kube-system kube-controller-manager-chadcrowell1c.mylabserver.com -o yaml
View the name of the kube-controller pod:

kubectl get pods -n kube-system
Set the VERSION variable to the latest stable release of Kubernetes:

export VERSION=v1.14.1
Set the ARCH variable to the amd64 system:

export ARCH=amd64
View the latest stable version of Kubernetes using the variable:

echo $VERSION
Curl the latest stable version of Kubernetes:

curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubeadm > kubeadm
Install the latest version of kubeadm:

sudo install -o root -g root -m 0755 ./kubeadm /usr/bin/kubeadm
Check the version of kubeadm:

sudo kubeadm version
Plan the upgrade:

sudo kubeadm upgrade plan
Apply the upgrade to 1.14.1:

kubeadm upgrade apply v1.14.1
View the differences between the old and new manifests:

diff kube-controller-manager.yaml /etc/kubernetes/manifests/kube-controller-manager.yaml
Curl the latest version of kubelet:

curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubelet > kubelet
Install the latest version of kubelet:

sudo install -o root -g root -m 0755 ./kubelet /usr/bin/kubelet
Restart the kubelet service:

sudo systemctl restart kubelet.service
Watch the nodes as they change version:

kubectl get nodes -w

See which pods are running on which nodes:

kubectl get pods -o wide
Evict the pods on a node:

kubectl drain [node_name] --ignore-daemonsets
Watch as the node changes status:

kubectl get nodes -w
Schedule pods to the node after maintenance is complete:

kubectl uncordon [node_name]
Remove a node from the cluster:

kubectl delete node [node_name]
Generate a new token:

sudo kubeadm token generate
List the tokens:

sudo kubeadm token list
Print the kubeadm join command to join a node to the cluster:

sudo kubeadm token create [token_name] --ttl 2h --print-join-command


Backing Up and Restoring a Kubernetes Cluster

Get the etcd binaries:

wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz
Unzip the compressed binaries:

tar xvf etcd-v3.3.12-linux-amd64.tar.gz
Move the files into /usr/local/bin:

sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin
Take a snapshot of the etcd datastore using etcdctl:

sudo ETCDCTL_API=3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key
View the help page for etcdctl:

ETCDCTL_API=3 etcdctl --help
Browse to the folder that contains the certificate files:

cd /etc/kubernetes/pki/etcd/
View that the snapshot was successful:

ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshot.db
Zip up the contents of the etcd directory:

sudo tar -zcvf etcd.tar.gz etcd
Copy the etcd directory to another server:

scp etcd.tar.gz cloud_user@18.219.235.42:~/


Pod and Node Networking

Kubernetes keeps networking simple for effective communication between pods, even if they are located on a different node. In this lesson, we’ll talk about pod communication from within a node, including how to inspect the virtual interfaces, and then get into what happens when a pod wants to talk to another pod on a different node.

See which node our pod is on:

kubectl get pods -o wide
Log in to the node:

ssh [node_name]
View the node's virtual network interfaces:

ifconfig
View the containers in the pod:

docker ps
Get the process ID for the container:

docker inspect --format '{{ .State.Pid }}' [container_id]
Use nsenter to run a command in the process's network namespace:

nsenter -t [container_pid] -n ip addr




When handling traffic from outside sources, there are two ways to direct that traffic to your pods: deploying a load balancer, and creating an ingress controller and an Ingress resource. In this lesson, we will talk about the benefits of each and how Kubernetes distributes traffic to the pods on a node to reduce latency and direct traffic to the appropriate services within your cluster.

View the list of services:

kubectl get services
The load balancer YAML spec:

apiVersion: v1
kind: Service
metadata:
  name: nginx-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: nginx
Create a new deployment:

kubectl run kubeserve2 --image=chadmcrowell/kubeserve2
View the list of deployments:

kubectl get deployments
Scale the deployments to 2 replicas:

kubectl scale deployment/kubeserve2 --replicas=2
View which pods are on which nodes:

kubectl get pods -o wide
Create a load balancer from a deployment:

kubectl expose deployment kubeserve2 --port 80 --target-port 8080 --type LoadBalancer
View the services in your cluster:

kubectl get services
Watch as an external port is created for a service:

kubectl get services -w
Look at the YAML for a service:

kubectl get services kubeserve2 -o yaml
Curl the external IP of the load balancer:

curl http://[external-ip]
View the annotation associated with a service:

kubectl describe services kubeserve
Set the annotation to route load balancer traffic local to the node:

kubectl annotate service kubeserve2 externalTrafficPolicy=Local
The YAML for an Ingress resource:

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: name-virtual-host-ingress
spec:
  rules:
  - host: first.bar.com
    http:
      paths:
      - backend:
          serviceName: service1
          servicePort: 80
  - host: second.foo.com
    http:
      paths:
      - backend:
          serviceName: service2
          servicePort: 80
  - http:
      paths:
      - backend:
          serviceName: service3
          servicePort: 80
Edit the ingress rules:

kubectl edit ingress
View the existing ingress rules:

kubectl describe ingress
Curl the hostname of your Ingress resource:

curl http://kubeserve2.example.com


CoreDNS is now the new default DNS plugin for Kubernetes. In this lesson, we’ll go over the hostnames for pods and services. We will also discover how you can customize DNS to include your own nameservers.

View the CoreDNS pods in the kube-system namespace:

kubectl get pods -n kube-system
View the CoreDNS deployment in your Kubernetes cluster:

kubectl get deployments -n kube-system
View the service that performs load balancing for the DNS server:

kubectl get services -n kube-system
Spec for the busybox pod:

apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox:1.28.4
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
View the resolv.conf file that contains the nameserver and search in DNS:

kubectl exec -it busybox -- cat /etc/resolv.conf
Look up the DNS name for the native Kubernetes service:

kubectl exec -it busybox -- nslookup kubernetes
Look up the DNS names of your pods:

kubectl exec -ti busybox -- nslookup [pod-ip-address].default.pod.cluster.local
Look up a service in your Kubernetes cluster:

kubectl exec -it busybox -- nslookup kube-dns.kube-system.svc.cluster.local
Get the logs of your CoreDNS pods:

kubectl logs [coredns-pod-name]
YAML spec for a headless service:

apiVersion: v1
kind: Service
metadata:
  name: kube-headless
spec:
  clusterIP: None
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubserve2
YAML spec for a custom DNS pod:

apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: dns-example
spec:
  containers:
    - name: test
      image: nginx
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
      - 8.8.8.8
    searches:
      - ns1.svc.cluster.local
      - my.dns.search.suffix
    options:
      - name: ndots
        value: "2"
      - name: edns0


The default scheduler in Kubernetes attempts to find the best node for your pod by going through a series of steps. In this lesson, we will cover the steps in detail in order to better understand the scheduler’s function when placing pods on nodes to maximize uptime for the applications running in your cluster. We will also go through how to create a deployment with node affinity.

Label your node as being located in availability zone 1:

kubectl label node chadcrowell1c.mylabserver.com availability-zone=zone1
Label your node as dedicated infrastructure:

kubectl label node chadcrowell1c.mylabserver.com share-type=dedicated
Here is the YAML for the deployment to include the node affinity rules:

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: pref
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: pref
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: availability-zone
                operator: In
                values:
                - zone1
          - weight: 20
            preference:
              matchExpressions:
              - key: share-type
                operator: In
                values:
                - dedicated
      containers:
      - args:
        - sleep
        - "99999"
        image: busybox
        name: main
Create the deployment:

kubectl create -f pref-deployment.yaml
View the deployment:

kubectl get deployments
View which pods landed on which nodes:

kubectl get pods -o wide



