KUBERNETES NOTES:


apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - image: busybox:1.28.4
    command:
      - sleep
      - "3600"
    name: busybox
  restartPolicy: Always



  kubectl get pods -o custom-columns=POD:metadata.name,NODE:spec.nodeNAME —sort-by spec.nodeNAME -n kube-system


  #curl localhost:8001/api/v1/namespaces/my-ns/services_project_name


  # cat /var/run/secrets/kubernetes.io/sericeaccount/token

  curl --head http://127.0.0.0.1:8081   #to curl the port forward running

  curl -I localhost:<node port>   # to view headers of the deployment

  View the version of the server and client on the master node:

kubectl version --short
View the version of the scheduler and controller manager:

kubectl get pods -n kube-system kube-controller-manager-chadcrowell1c.mylabserver.com -o yaml
View the name of the kube-controller pod:

kubectl get pods -n kube-system
Set the VERSION variable to the latest stable release of Kubernetes:

export VERSION=v1.14.1
Set the ARCH variable to the amd64 system:

export ARCH=amd64
View the latest stable version of Kubernetes using the variable:

echo $VERSION
Curl the latest stable version of Kubernetes:

curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubeadm > kubeadm
Install the latest version of kubeadm:

sudo install -o root -g root -m 0755 ./kubeadm /usr/bin/kubeadm
Check the version of kubeadm:

sudo kubeadm version
Plan the upgrade:

sudo kubeadm upgrade plan
Apply the upgrade to 1.14.1:

kubeadm upgrade apply v1.14.1
View the differences between the old and new manifests:

diff kube-controller-manager.yaml /etc/kubernetes/manifests/kube-controller-manager.yaml
Curl the latest version of kubelet:

curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubelet > kubelet
Install the latest version of kubelet:

sudo install -o root -g root -m 0755 ./kubelet /usr/bin/kubelet
Restart the kubelet service:

sudo systemctl restart kubelet.service
Watch the nodes as they change version:

kubectl get nodes -w

See which pods are running on which nodes:

kubectl get pods -o wide
Evict the pods on a node:

kubectl drain [node_name] --ignore-daemonsets
Watch as the node changes status:

kubectl get nodes -w
Schedule pods to the node after maintenance is complete:

kubectl uncordon [node_name]
Remove a node from the cluster:

kubectl delete node [node_name]
Generate a new token:

sudo kubeadm token generate
List the tokens:

sudo kubeadm token list
Print the kubeadm join command to join a node to the cluster:

sudo kubeadm token create [token_name] --ttl 2h --print-join-command


Backing Up and Restoring a Kubernetes Cluster
========================================================


Get the etcd binaries:

wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz
Unzip the compressed binaries:

tar xvf etcd-v3.3.12-linux-amd64.tar.gz
Move the files into /usr/local/bin:

sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin
Take a snapshot of the etcd datastore using etcdctl:

sudo ETCDCTL_API=3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key
View the help page for etcdctl:

ETCDCTL_API=3 etcdctl --help
Browse to the folder that contains the certificate files:

cd /etc/kubernetes/pki/etcd/
View that the snapshot was successful:

ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshot.db
Zip up the contents of the etcd directory:

sudo tar -zcvf etcd.tar.gz etcd
Copy the etcd directory to another server:

scp etcd.tar.gz cloud_user@18.219.235.42:~/


Pod and Node Networking
==========================================


Kubernetes keeps networking simple for effective communication between pods, even if they are located on a different node. In this lesson, we’ll talk about pod communication from within a node, including how to inspect the virtual interfaces, and then get into what happens when a pod wants to talk to another pod on a different node.

See which node our pod is on:

kubectl get pods -o wide
Log in to the node:

ssh [node_name]
View the node's virtual network interfaces:

ifconfig
View the containers in the pod:

docker ps
Get the process ID for the container:

docker inspect --format '{{ .State.Pid }}' [container_id]
Use nsenter to run a command in the process's network namespace:

nsenter -t [container_pid] -n ip addr




When handling traffic from outside sources, there are two ways to direct that traffic to your pods: deploying a load balancer, and creating an ingress controller and an Ingress resource. In this lesson, we will talk about the benefits of each and how Kubernetes distributes traffic to the pods on a node to reduce latency and direct traffic to the appropriate services within your cluster.

View the list of services:

kubectl get services
The load balancer YAML spec:

apiVersion: v1
kind: Service
metadata:
  name: nginx-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: nginx
Create a new deployment:

kubectl run kubeserve2 --image=chadmcrowell/kubeserve2
View the list of deployments:

kubectl get deployments
Scale the deployments to 2 replicas:

kubectl scale deployment/kubeserve2 --replicas=2
View which pods are on which nodes:

kubectl get pods -o wide
Create a load balancer from a deployment:

kubectl expose deployment kubeserve2 --port 80 --target-port 8080 --type LoadBalancer
View the services in your cluster:

kubectl get services
Watch as an external port is created for a service:

kubectl get services -w
Look at the YAML for a service:

kubectl get services kubeserve2 -o yaml
Curl the external IP of the load balancer:

curl http://[external-ip]
View the annotation associated with a service:

kubectl describe services kubeserve
Set the annotation to route load balancer traffic local to the node:

kubectl annotate service kubeserve2 externalTrafficPolicy=Local
The YAML for an Ingress resource:

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: name-virtual-host-ingress
spec:
  rules:
  - host: first.bar.com
    http:
      paths:
      - backend:
          serviceName: service1
          servicePort: 80
  - host: second.foo.com
    http:
      paths:
      - backend:
          serviceName: service2
          servicePort: 80
  - http:
      paths:
      - backend:
          serviceName: service3
          servicePort: 80
Edit the ingress rules:

kubectl edit ingress
View the existing ingress rules:

kubectl describe ingress
Curl the hostname of your Ingress resource:

curl http://kubeserve2.example.com


CoreDNS is now the new default DNS plugin for Kubernetes. In this lesson, we’ll go over the hostnames for pods and services. We will also discover how you can customize DNS to include your own nameservers.

View the CoreDNS pods in the kube-system namespace:

kubectl get pods -n kube-system
View the CoreDNS deployment in your Kubernetes cluster:

kubectl get deployments -n kube-system
View the service that performs load balancing for the DNS server:

kubectl get services -n kube-system
Spec for the busybox pod:

apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox:1.28.4
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
View the resolv.conf file that contains the nameserver and search in DNS:

kubectl exec -it busybox -- cat /etc/resolv.conf
Look up the DNS name for the native Kubernetes service:

kubectl exec -it busybox -- nslookup kubernetes
Look up the DNS names of your pods:

kubectl exec -ti busybox -- nslookup [pod-ip-address].default.pod.cluster.local
Look up a service in your Kubernetes cluster:

kubectl exec -it busybox -- nslookup kube-dns.kube-system.svc.cluster.local
Get the logs of your CoreDNS pods:

kubectl logs [coredns-pod-name]
YAML spec for a headless service:

apiVersion: v1
kind: Service
metadata:
  name: kube-headless
spec:
  clusterIP: None
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubserve2
YAML spec for a custom DNS pod:

apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: dns-example
spec:
  containers:
    - name: test
      image: nginx
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
      - 8.8.8.8
    searches:
      - ns1.svc.cluster.local
      - my.dns.search.suffix
    options:
      - name: ndots
        value: "2"
      - name: edns0


The default scheduler in Kubernetes attempts to find the best node for your pod by going through a series of steps. In this lesson, we will cover the steps in detail in order to better understand the scheduler’s function when placing pods on nodes to maximize uptime for the applications running in your cluster. We will also go through how to create a deployment with node affinity.

Label your node as being located in availability zone 1:

kubectl label node chadcrowell1c.mylabserver.com availability-zone=zone1
Label your node as dedicated infrastructure:

kubectl label node chadcrowell1c.mylabserver.com share-type=dedicated
Here is the YAML for the deployment to include the node affinity rules:

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: pref
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: pref
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: availability-zone
                operator: In
                values:
                - zone1
          - weight: 20
            preference:
              matchExpressions:
              - key: share-type
                operator: In
                values:
                - dedicated
      containers:
      - args:
        - sleep
        - "99999"
        image: busybox
        name: main
Create the deployment:

kubectl create -f pref-deployment.yaml
View the deployment:

kubectl get deployments
View which pods landed on which nodes:

kubectl get pods -o wide


Scheduling pods with Resource limits and label selectors:
==============================================================



In order to share the resources of your node properly, you can set resource limits and requests in Kubernetes. This allows you to reserve enough CPU and memory for your application while still maintaining system health. In this lesson, we will create some requests and limits in our pod YAML to show how it’s used by the node.

View the capacity and the allocatable info from a node:

kubectl describe nodes
The pod YAML for a pod with requests:

apiVersion: v1
kind: Pod
metadata:
  name: resource-pod1
spec:
  nodeSelector:
    kubernetes.io/hostname: "chadcrowell3c.mylabserver.com"
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: pod1
    resources:
      requests:
        cpu: 800m
        memory: 20Mi
Create the requests pod:

kubectl create -f resource-pod1.yaml
View the pods and nodes they landed on:

kubectl get pods -o wide
The YAML for a pod that has a large request:

apiVersion: v1
kind: Pod
metadata:
  name: resource-pod2
spec:
  nodeSelector:
    kubernetes.io/hostname: "chadcrowell3c.mylabserver.com"
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: pod2
    resources:
      requests:
        cpu: 1000m
        memory: 20Mi
Create the pod with 1000 millicore request:

kubectl create -f resource-pod2.yaml
See why the pod with a large request didn’t get scheduled:

kubectl describe resource-pod2
Look at the total requests per node:

kubectl describe nodes chadcrowell3c.mylabserver.com
Delete the first pod to make room for the pod with a large request:

kubectl delete pods resource-pod1
Watch as the first pod is terminated and the second pod is started:

kubectl get pods -o wide -w
The YAML for a pod that has limits:

apiVersion: v1
kind: Pod
metadata:
  name: limited-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      limits:
        cpu: 1
        memory: 20Mi
Create a pod with limits:

kubectl create -f limited-pod.yaml
Use the exec utility to use the top command:

kubectl exec -it limited-pod top
Helpful Links
Configure Default CPU Requests and Limits
Configure Default Memory Requests and Limits





DaemonSets do not use a scheduler to deploy pods. In fact, there are currently DaemonSets in the Kubernetes cluster that we made. In this lesson, I will show you where to find those and how to create your own DaemonSet pods to deploy without the need for a scheduler.

Find the DaemonSet pods that exist in your kubeadm cluster:

kubectl get pods -n kube-system -o wide
Delete a DaemonSet pod and see what happens:

kubectl delete pods [pod_name] -n kube-system
Give the node a label to signify it has SSD:

kubectl label node[node_name] disk=ssd
The YAML for a DaemonSet:

apiVersion: apps/v1beta2
kind: DaemonSet
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: linuxacademycontent/ssd-monitor
Create a DaemonSet from a YAML spec:

kubectl create -f ssd-monitor.yaml
Label another node to specify it has SSD:

kubectl label node chadcrowell2c.mylabserver.com disk=ssd
View the DaemonSet pods that have been deployed:

kubectl get pods -o wide
Remove the label from a node and watch the DaemonSet pod terminate:

kubectl label node chadcrowell3c.mylabserver.com disk-
Change the label on a node to change it to spinning disk:

kubectl label node chadcrowell2c.mylabserver.com disk=hdd --overwrite
Pick the label to choose for your DaemonSet:

kubectl get nodes chadcrowell3c.mylabserver.com --show-labels
Helpful Links
DaemonSets

Displaying scheduler Events
=========================================



There are multiple ways to view the events related to the scheduler. In this lesson, we’ll look at ways in which you can troubleshoot any problems with your scheduler or just find out more information.

View the name of the scheduler pod:

kubectl get pods -n kube-system
Get the information about your scheduler pod events:

kubectl describe pods [scheduler_pod_name] -n kube-system
View the events in your default namespace:

kubectl get events
View the events in your kube-system namespace:

kubectl get events -n kube-system
Delete all the pods in your default namespace:

kubectl delete pods --all
Watch events as they are appearing in real time:

kubectl get events -w
View the logs from the scheduler pod:

kubectl logs [kube_scheduler_pod_name] -n kube-system
The location of a systemd service scheduler pod:

/var/log/kube-scheduler.log


##################################################################


Deploying Application,rolling updates
=============================================



We already know Kubernetes will run pods and deployments, but what happens when you need to update or change the version of your application running inside of the Kubernetes cluster? That’s where rolling updates come in, allowing you to update the app image with zero downtime. In this lesson, we’ll go over a rolling update, how to roll back, and how to pause the update if things aren’t going well.

The YAML for a deployment:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      containers:
      - image: linuxacademycontent/kubeserve:v1
        name: app
Create a deployment with a record (for rollbacks):

kubectl create -f kubeserve-deployment.yaml --record
Check the status of the rollout:

kubectl rollout status deployments kubeserve
View the ReplicaSets in your cluster:

kubectl get replicasets
Scale up your deployment by adding more replicas:

kubectl scale deployment kubeserve --replicas=5
Expose the deployment and provide it a service:

kubectl expose deployment kubeserve --port 80 --target-port 80 --type NodePort
Set the minReadySeconds attribute to your deployment:

kubectl patch deployment kubeserve -p '{"spec": {"minReadySeconds": 10}}'
Use kubectl apply to update a deployment:

kubectl apply -f kubeserve-deployment.yaml
Use kubectl replace to replace an existing deployment:

kubectl replace -f kubeserve-deployment.yaml
Run this curl look while the update happens:

while true; do curl http://10.105.31.119; done
Perform the rolling update:

kubectl set image deployments/kubeserve app=linuxacademycontent/kubeserve:v2 --v 6
Describe a certain ReplicaSet:

kubectl describe replicasets kubeserve-[hash]
Apply the rolling update to version 3 (buggy):

kubectl set image deployment kubeserve app=linuxacademycontent/kubeserve:v3
Undo the rollout and roll back to the previous version:

kubectl rollout undo deployments kubeserve
Look at the rollout history:

kubectl rollout history deployment kubeserve
Roll back to a certain revision:

kubectl rollout undo deployment kubeserve --to-revision=2
Pause the rollout in the middle of a rolling update (canary release):

kubectl rollout pause deployment kubeserve
Resume the rollout after the rolling update looks good:

kubectl rollout resume deployment kubeserve



#########################################################################################

Application HA , configmap and secrets
==========================================



Continuing from the last lesson, we will go through how Kubernetes will save you from EVER releasing code with bugs. Then, we will talk about ConfigMaps and secrets as a way to pass configuration data to your apps.

The YAML for a readiness probe:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubeserve
  minReadySeconds: 10
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      containers:
      - image: linuxacademycontent/kubeserve:v3
        name: app
        readinessProbe:
          periodSeconds: 1
          httpGet:
            path: /
            port: 80
Apply the readiness probe:

kubectl apply -f kubeserve-deployment-readiness.yaml
View the rollout status:

kubectl rollout status deployment kubeserve
Describe deployment:

kubectl describe deployment
Create a ConfigMap with two keys:

kubectl create configmap appconfig --from-literal=key1=value1 --from-literal=key2=value2
Get the YAML back out from the ConfigMap:

kubectl get configmap appconfig -o yaml
The YAML for the ConfigMap pod:

apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
  - name: app-container
    image: busybox:1.28
    command: ['sh', '-c', "echo $(MY_VAR) && sleep 3600"]
    env:
    - name: MY_VAR
      valueFrom:
        configMapKeyRef:
          name: appconfig
          key: key1
Create the pod that is passing the ConfigMap data:

kubectl apply -f configmap-pod.yaml
Get the logs from the pod displaying the value:

kubectl logs configmap-pod
The YAML for a pod that has a ConfigMap volume attached:

apiVersion: v1
kind: Pod
metadata:
  name: configmap-volume-pod
spec:
  containers:
  - name: app-container
    image: busybox
    command: ['sh', '-c', "echo $(MY_VAR) && sleep 3600"]
    volumeMounts:
      - name: configmapvolume
        mountPath: /etc/config
  volumes:
    - name: configmapvolume
      configMap:
        name: appconfig
Create the ConfigMap volume pod:

kubectl apply -f configmap-volume-pod.yaml
Get the keys from the volume on the container:

kubectl exec configmap-volume-pod -- ls /etc/config
Get the values from the volume on the pod:

kubectl exec configmap-volume-pod -- cat /etc/config/key1
The YAML for a secret:

apiVersion: v1
kind: Secret
metadata:
  name: appsecret
stringData:
  cert: value
  key: value
Create the secret:

kubectl apply -f appsecret.yaml
The YAML for a pod that will use the secret:

apiVersion: v1
kind: Pod
metadata:
  name: secret-pod
spec:
  containers:
  - name: app-container
    image: busybox
    command: ['sh', '-c', "echo Hello, Kubernetes! && sleep 3600"]
    env:
    - name: MY_CERT
      valueFrom:
        secretKeyRef:
          name: appsecret
          key: cert
Create the pod that has attached secret data:

kubectl apply -f secret-pod.yaml
Open a shell and echo the environment variable:

kubectl exec -it secret-pod -- sh
echo $MY_CERT
The YAML for a pod that will access the secret from a volume:

apiVersion: v1
kind: Pod
metadata:
  name: secret-volume-pod
spec:
  containers:
  - name: app-container
    image: busybox
    command: ['sh', '-c', "echo $(MY_VAR) && sleep 3600"]
    volumeMounts:
      - name: secretvolume
        mountPath: /etc/certs
  volumes:
    - name: secretvolume
      secret:
        secretName: appsecret
Create the pod with volume attached with secrets:

kubectl apply -f secret-volume-pod.yaml
Get the keys from the volume mounted to the container with the secrets:

kubectl exec secret-volume-pod -- ls /etc/certs




Creating self healing application:
=============================================



In this lesson, we’ll go through the power of ReplicaSets, which make your application self-healing by replicating pods and moving them around and spinning them up when nodes fail. We’ll also talk about StatefulSets and the benefit they provide.

The YAML for a ReplicaSet:

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myreplicaset
  labels:
    app: app
    tier: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: main
        image: linuxacademycontent/kubeserve
Create the ReplicaSet:

kubectl apply -f replicaset.yaml
The YAML for a pod with the same label as a ReplicaSet:

apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    tier: frontend
spec:
  containers:
  - name: main
    image: linuxacademycontent/kubeserve
Create the pod with the same label:

kubectl apply -f pod-replica.yaml
Watch the pod get terminated:

kubectl get pods -w 
The YAML for a StatefulSet:

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
Create the StatefulSet:

kubectl apply -f statefulset.yaml
View all StatefulSets in the cluster:

kubectl get statefulsets
Describe the StatefulSets:

kubectl describe statefulsets


self healing applications
==============================



In this lesson, we’ll go through the power of ReplicaSets, which make your application self-healing by replicating pods and moving them around and spinning them up when nodes fail. We’ll also talk about StatefulSets and the benefit they provide.

The YAML for a ReplicaSet:

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myreplicaset
  labels:
    app: app
    tier: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: main
        image: linuxacademycontent/kubeserve
Create the ReplicaSet:

kubectl apply -f replicaset.yaml
The YAML for a pod with the same label as a ReplicaSet:

apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    tier: frontend
spec:
  containers:
  - name: main
    image: linuxacademycontent/kubeserve
Create the pod with the same label:

kubectl apply -f pod-replica.yaml
Watch the pod get terminated:

kubectl get pods -w 
The YAML for a StatefulSet:

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
Create the StatefulSet:

kubectl apply -f statefulset.yaml
View all StatefulSets in the cluster:

kubectl get statefulsets
Describe the StatefulSets:

kubectl describe statefulsets


Persistance Volumes
============================


In Kubernetes, pods are ephemeral. This creates a unique challenge with attaching storage directly to the filesystem of a container. Persistent Volumes are used to create an abstraction layer between the application and the underlying storage, making it easier for the storage to follow the pods as they are deleted, moved, and created within your Kubernetes cluster.

In the Google Cloud Engine, find the region your cluster is in:

gcloud container clusters list
Using Google Cloud, create a persistent disk in the same region as your cluster:

gcloud compute disks create --size=1GiB --zone=us-central1-a mongodb
The YAML for a pod that will use persistent disk:

apiVersion: v1
kind: Pod
metadata:
  name: mongodb 
spec:
  volumes:
  - name: mongodb-data
    gcePersistentDisk:
      pdName: mongodb
      fsType: ext4
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
Create the pod with disk attached and mounted:

kubectl apply -f mongodb-pod.yaml
See which node the pod landed on:

kubectl get pods -o wide
Connect to the mongodb shell:

kubectl exec -it mongodb mongo
Switch to the mystore database in the mongodb shell:

use mystore
Create a JSON document to insert into the database:

db.foo.insert({name:'foo'})
View the document you just created:

db.foo.find()
Exit from the mongodb shell:

exit
Delete the pod:

kubectl delete pod mongodb
Create a new pod with the same attached disk:

kubectl apply -f mongodb-pod.yaml
Check to see which node the pod landed on:

kubectl get pods -o wide
Drain the node (if the pod is on the same node as before):

kubectl drain [node_name] --ignore-daemonsets
Once the pod is on a different node, access the mongodb shell again:

kubectl exec -it mongodb mongo
Access the mystore database again:

use mystore
Find the document you created from before:

db.foo.find()
The YAML for a PersistentVolume object in Kubernetes:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  capacity: 
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
    - ReadOnlyMany
  persistentVolumeReclaimPolicy: Retain
  gcePersistentDisk:
    pdName: mongodb
    fsType: ext4
Create the Persistent Volume resource:

kubectl apply -f mongodb-persistentvolume.yaml
View our Persistent Volumes:

kubectl get persistentvolumes


Volume Access Modes:
=============================


Volume access modes are how you specify the access of a node to your Persistent Volume. There are three types of access modes: ReadWriteOnce, ReadOnlyMany, and ReadWriteMany. In this lesson, we will explain what each of these access modes means and two VERY IMPORTANT things to remember when using your Persistent Volumes with pods.

The YAML for a Persistent Volume:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  capacity: 
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
    - ReadOnlyMany
  persistentVolumeReclaimPolicy: Retain
  gcePersistentDisk:
    pdName: mongodb
    fsType: ext4
View the Persistent Volumes in your cluster:

kubectl get pv


Persistance Volume claim (PVC)
========================================


Persistent Volume Claims (PVCs) are a way for an application developer to request storage for the application without having to know where the underlying storage is. The claim is then bound to the Persistent Volume (PV), and it will not be released until the PVC is deleted. In this lesson, we will go through creating a PVC and accessing storage within our persistent disk.

The YAML for a PVC:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc 
spec:
  resources:
    requests:
      storage: 1Gi
  accessModes:
  - ReadWriteOnce
  storageClassName: ""
Create a PVC:

kubectl apply -f mongodb-pvc.yaml
View the PVC in the cluster:

kubectl get pvc
View the PV to ensure it’s bound:

kubectl get pv
The YAML for a pod that uses a PVC:

apiVersion: v1
kind: Pod
metadata:
  name: mongodb 
spec:
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
  volumes:
  - name: mongodb-data
    persistentVolumeClaim:
      claimName: mongodb-pvc
Create the pod with the attached storage:

kubectl apply -f mongo-pvc-pod.yaml
Access the mogodb shell:

kubectl exec -it mongodb mongo
Find the JSON document created in previous lessons:

db.foo.find()
Delete the mongodb pod:

kubectl delete pod mogodb
Delete the mongodb-pvc PVC:

kubectl delete pvc mongodb-pvc
Check the status of the PV:

kubectl get pv
The YAML for the PV to show its reclaim policy:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  capacity: 
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
    - ReadOnlyMany
  persistentVolumeReclaimPolicy: Retain
  gcePersistentDisk:
    pdName: mongodb
    fsType: ext4


Storage Objects:
=====================



There’s an even easier way to provision storage in Kubernetes with StorageClass objects. Also, your storage is safe from data loss with the “Storage Object in Use Protection” feature, which ensures any pods using a Persistent Volume will not lose the data on the volume as long as it is actively mounted. We’ve been using Google Storage for this section, but there are many different volume types you can use in Kubernetes. In this lesson, we will talk about the hostPath volume and the empty directory volume type.

See the PV protection on your volume:

kubectl describe pv mongodb-pv
See the PVC protection for your claim:

kubectl describe pvc mongodb-pvc
Delete the PVC:

kubectl delete pvc mongodb-pvc
See that the PVC is terminated, but the volume is still attached to pod:

kubectl get pvc
Try to access the data, even though we just deleted the PVC:

kubectl exec -it mongodb mongo
use mystore
db.foo.find()
Delete the pod, which finally deletes the PVC:

kubectl delete pods mongodb
Show that the PVC is deleted:

kubectl get pvc
YAML for a StorageClass object:

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
Create the StorageClass type "fast":

kubectl apply -f sc-fast.yaml
Change the PVC to include the new StorageClass object:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc 
spec:
  storageClassName: fast
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce
Create the PVC with automatically provisioned storage:

kubectl apply -f mongodb-pvc.yaml
View the PVC with new StorageClass:

kubectl get pvc
View the newly provisioned storage:

kubectl get pv
The YAML for a hostPath PV:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
spec:
  storageClassName: local-storage
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
The YAML for a pod with an empty directory volume:

apiVersion: v1
kind: Pod
metadata:
  name: emptydir-pod
spec:
  containers:
  - image: busybox
    name: busybox
    command: ["/bin/sh", "-c", "while true; do sleep 3600; done"]
    volumeMounts:
    - mountPath: /tmp/storage
      name: vol
  volumes:
  - name: vol
    emptyDir: {}



Application with persistance storage:
=======================================================

In this lesson, we’ll wrap everything up in a nice little bow and create a deployment that will allow us to use our storage with our pods. This is to demonstrate how a real-world application would be deployed and used for storing data.

The YAML for our StorageClass object:

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
The YAML for our PVC:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kubeserve-pvc 
spec:
  storageClassName: fast
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce
Create our StorageClass object:

kubectl apply -f storageclass-fast.yaml
View the StorageClass objects in your cluster:

kubectl get sc
Create our PVC:

kubectl apply -f kubeserve-pvc.yaml
View the PVC created in our cluster:

kubectl get pvc
View our automatically provisioned PV:

kubectl get pv
The YAML for our deployment:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      containers:
      - env:
        - name: app
          value: "1"
        image: linuxacademycontent/kubeserve:v1
        name: app
        volumeMounts:
        - mountPath: /data
          name: volume-data
      volumes:
      - name: volume-data
        persistentVolumeClaim:
          claimName: kubeserve-pvc
Create our deployment and attach the storage to the pods:

kubectl apply -f kubeserve-deployment.yaml
Check the status of the rollout:

kubectl rollout status deployments kubeserve
Check the pods have been created:

kubectl get pods
Connect to our pod and create a file on the PV:

kubectl exec -it [pod-name] -- touch /data/file1.txt
Connect to our pod and list the contents of the /data directory:

kubectl exec -it [pod-name] -- ls /data


Lab:
Creating Persistent Storage for Pods in Kubernetes
=======================================================


In this hands-on lab, to decouple our storage from our pods, we will create a persistent volume to mount for use by our pods. We will deploy a mongodb image that will contain a MongoDB database. We will first create the persistent volume, then create the pod YAML for deploying the pod to mount the volume. We will then delete the pod and create a new pod, which will access that same volume.

Log in to the Kube Master server using the credentials on the lab page (either in your local terminal, using the Instant Terminal feature, or using the public IP), and work through the objectives listed.

Create a PersistentVolume.
Use the following YAML spec for the PersistentVolume named mongodb-pv.yaml:

 apiVersion: v1
 kind: PersistentVolume
 metadata:
   name: mongodb-pv
 spec:
   storageClassName: local-storage
   capacity:
     storage: 1Gi
   accessModes:
     - ReadWriteOnce
   hostPath:
     path: "/mnt/data"


Then, create the PersistentVolume:

 kubectl apply -f mongodb-pv.yaml
Create a PersistentVolumeClaim.
Use the following YAML spec for the PersistentVolumeClaim named mongodb-pvc.yaml:

 apiVersion: v1
 kind: PersistentVolumeClaim
 metadata:
   name: mongodb-pvc
 spec:
   storageClassName: local-storage
   accessModes:
     - ReadWriteOnce
   resources:
     requests:
       storage: 1Gi
Then, create the PersistentVolumeClaim:

 kubectl apply -f mongodb-pvc.yaml
Create a pod from the mongodb image, with a mounted volume to mount path /data/db.
Use the following YAML spec for the pod named mongodb-pod.yaml:

 apiVersion: v1
 kind: Pod
 metadata:
   name: mongodb
 spec:
   containers:
   - image: mongo
     name: mongodb
     volumeMounts:
     - name: mongodb-data
       mountPath: /data/db
     ports:
     - containerPort: 27017
       protocol: TCP
   volumes:
   - name: mongodb-data
     persistentVolumeClaim:
       claimName: mongodb-pvc
Then, create the pod:

 kubectl apply -f mongodb-pod.yaml
Verify the pod was created:

 kubectl get pods
Access the node and view the data within the volume.
Run the following command:

 kubectl get nodes
Connect to the worker node (get the <node_hostname> from the NAME column of the above output), using the same password as the Kube Master:

 ssh <node_hostname>
Switch to the /mnt/data directory:

 cd /mnt/data
List the contents of the directory:

 ls
Delete the pod and create a new pod with the same YAML spec.
Exit out of the worker node:

 exit
Delete the pod:

 kubectl delete pod mongodb
Create a new pod:

 kubectl apply -f mongodb-pod.yaml
Verify the data still resides on the volume.
Log in to the worker node again:

 ssh <node_hostname>
Switch to the /mnt/data directory:

 cd /mnt/data
List the contents of the directory:

 ls



Monitoring the Cluster Components
=============================================================


KUBERNETES THE HARDWAY:
=================================


Provisioning of CA Certificate:
=====================================


In order to generate the certificates needed by Kubernetes, you must first provision a certificate authority. This lesson will guide you through the process of provisioning a new certificate authority for your Kubernetes cluster. After completing this lesson, you should have a certificate authority, which consists of two files: ca-key.pem and ca.pem.

Here are the commands used in the demo:

cd ~/
mkdir kthw
cd kthw/
UPDATE: cfssljson and cfssl will need to be installed. To install, complete the following commands:

sudo curl -s -L -o /bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
sudo curl -s -L -o /bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
sudo curl -s -L -o /bin/cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
sudo chmod +x /bin/cfssl*
Use this command to generate the certificate authority. Include the opening and closing curly braces to run this entire block as a single command.

{

cat > ca-config.json << EOF
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "8760h"
      }
    }
  }
}
EOF

cat > ca-csr.json << EOF
{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "CA",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert -initca ca-csr.json | cfssljson -bare ca

}



Generate Client certificate:

1) Admin client certificate

2) Kubelet client certificate (  this will be for each worker server)

3) Generate for kube controller mgr

4)kubbe proxy

5) kube scheduler


example:

Now that you have provisioned a certificate authority for the Kubernetes cluster, you are ready to begin generating certificates. The first set of certificates you will need to generate consists of the client certificates used by various Kubernetes components. In this lesson, we will generate the following client certificates: admin, kubelet (one for each worker node), kube-controller-manager, kube-proxy, and kube-scheduler. After completing this lesson, you will have the client certificate files which you will need later to set up the cluster.

Here are the commands used in the demo. The command blocks surrounded by curly braces can be entered as a single command:

cd ~/kthw
Admin Client certificate:

{

cat > admin-csr.json << EOF
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:masters",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin

}
Kubelet Client certificates. Be sure to enter your actual cloud server values for all four of the variables at the top:

WORKER0_HOST=<Public hostname of your first worker node cloud server>
WORKER0_IP=<Private IP of your first worker node cloud server>
WORKER1_HOST=<Public hostname of your second worker node cloud server>
WORKER1_IP=<Private IP of your second worker node cloud server>

{
cat > ${WORKER0_HOST}-csr.json << EOF
{
  "CN": "system:node:${WORKER0_HOST}",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:nodes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${WORKER0_IP},${WORKER0_HOST} \
  -profile=kubernetes \
  ${WORKER0_HOST}-csr.json | cfssljson -bare ${WORKER0_HOST}

cat > ${WORKER1_HOST}-csr.json << EOF
{
  "CN": "system:node:${WORKER1_HOST}",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:nodes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${WORKER1_IP},${WORKER1_HOST} \
  -profile=kubernetes \
  ${WORKER1_HOST}-csr.json | cfssljson -bare ${WORKER1_HOST}

}


Generating the Kubernetes API Server Certificate

HANDS ON:
=============


Building a Kubernetes Cluster with Kubeadm
Introduction
A Kubernetes cluster is a powerful tool for managing containers in a highly-available manner. Kubeadm greatly simplifies the process of setting up a simple cluster. In this hands-on lab, you will build your own working Kubernetes cluster using Kubeadm.

Solution
Begin by logging in to the lab servers using the credentials provided on the hands-on lab page:

Install Docker on all three nodes
====================================



ssh cloud_user@PUBLIC_IP_ADDRESS
Install Docker on all three nodes
Do the following on all three nodes:

Add the Docker GPG key:

 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
Add the Docker repository:

 sudo add-apt-repository \
    "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
    $(lsb_release -cs) \
    stable"
Update packages:

 sudo apt-get update
Install Docker:

 sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu
Hold Docker at this specific version:

 sudo apt-mark hold docker-ce
Verify that Docker is up and running with:

 sudo systemctl status docker
Make sure the Docker service status is active (running)!

Install Kubeadm, Kubelet, and Kubectl on all three nodes
==================================================================
Install the Kubernetes components by running this on all three nodes:

Add the Kubernetes GPG key:

 curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
Add the Kubernetes repository:

 cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
 deb https://apt.kubernetes.io/ kubernetes-xenial main
 EOF
Update packages:

 sudo apt-get update
Install kubelet, kubeadm, and kubectl:

 sudo apt-get install -y kubelet=1.12.7-00 kubeadm=1.12.7-00 kubectl=1.12.7-00
Hold the Kubernetes components at this specific version:

 sudo apt-mark hold kubelet kubeadm kubectl


Bootstrap the cluster on the Kube master node
=============================================================

On the Kube master node, do this:

 sudo kubeadm init --pod-network-cidr=10.244.0.0/16
That command may take a few minutes to complete.

When it is done, set up the local kubeconfig:

 mkdir -p $HOME/.kube
 sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
 sudo chown $(id -u):$(id -g) $HOME/.kube/config
Take note that the kubeadm init command printed a long kubeadm join command to the screen. You will need that kubeadm join command in the next step!

Run the following command on the Kube master node to verify it is up and running:

 kubectl version
This command should return both a Client Version and a Server Version.

Join the two Kube worker nodes to the cluster
=============================================================


Copy the kubeadm join command that was printed by the kubeadm init command earlier, with the token and hash. Run this command on both worker nodes, but make sure you add sudo in front of it:

 sudo kubeadm join $some_ip:6443 --token $some_token --discovery-token-ca-cert-hash $some_hash
Now, on the Kube master node, make sure your nodes joined the cluster successfully:

 kubectl get nodes
Verify that all three of your nodes are listed. It will look something like this:

 NAME            STATUS     ROLES    AGE   VERSION
 ip-10-0-1-101   NotReady   master   30s   v1.12.2
 ip-10-0-1-102   NotReady   <none>   8s    v1.12.2
 ip-10-0-1-103   NotReady   <none>   5s    v1.12.2
Note that the nodes are expected to be in the NotReady state for now.

Set up cluster networking with flannel
=======================================================


Turn on iptables bridge calls on all three nodes:

 echo "net.bridge.bridge-nf-call-iptables=1" | sudo tee -a /etc/sysctl.conf
 sudo sysctl -p
Next, run this only on the Kube master node:

 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml
Now flannel is installed! Make sure it is working by checking the node status again:

 kubectl get nodes
After a short time, all three nodes should be in the Ready state. If they are not all Ready the first time you run kubectl get nodes, wait a few moments and try again. It should look something like this:

 NAME            STATUS   ROLES    AGE   VERSION
 ip-10-0-1-101   Ready    master   85s   v1.12.2
 ip-10-0-1-102   Ready    <none>   63s   v1.12.2
 ip-10-0-1-103   Ready    <none>   60s   v1.12.2



