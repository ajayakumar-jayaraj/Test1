KUBERNETES NOTES:


apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - image: busybox:1.28.4
    command:
      - sleep
      - "3600"
    name: busybox
  restartPolicy: Always



  kubectl get pods -o custom-columns=POD:metadata.name,NODE:spec.nodeNAME —sort-by spec.nodeNAME -n kube-system


  #curl localhost:8001/api/v1/namespaces/my-ns/services_project_name


  # cat /var/run/secrets/kubernetes.io/sericeaccount/token

  curl --head http://127.0.0.0.1:8081   #to curl the port forward running

  curl -I localhost:<node port>   # to view headers of the deployment

  View the version of the server and client on the master node:

kubectl version --short
View the version of the scheduler and controller manager:

kubectl get pods -n kube-system kube-controller-manager-chadcrowell1c.mylabserver.com -o yaml
View the name of the kube-controller pod:

kubectl get pods -n kube-system
Set the VERSION variable to the latest stable release of Kubernetes:

export VERSION=v1.14.1
Set the ARCH variable to the amd64 system:

export ARCH=amd64
View the latest stable version of Kubernetes using the variable:

echo $VERSION
Curl the latest stable version of Kubernetes:

curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubeadm > kubeadm
Install the latest version of kubeadm:

sudo install -o root -g root -m 0755 ./kubeadm /usr/bin/kubeadm
Check the version of kubeadm:

sudo kubeadm version
Plan the upgrade:

sudo kubeadm upgrade plan
Apply the upgrade to 1.14.1:

kubeadm upgrade apply v1.14.1
View the differences between the old and new manifests:

diff kube-controller-manager.yaml /etc/kubernetes/manifests/kube-controller-manager.yaml
Curl the latest version of kubelet:

curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubelet > kubelet
Install the latest version of kubelet:

sudo install -o root -g root -m 0755 ./kubelet /usr/bin/kubelet
Restart the kubelet service:

sudo systemctl restart kubelet.service
Watch the nodes as they change version:

kubectl get nodes -w

See which pods are running on which nodes:

kubectl get pods -o wide
Evict the pods on a node:

kubectl drain [node_name] --ignore-daemonsets
Watch as the node changes status:

kubectl get nodes -w
Schedule pods to the node after maintenance is complete:

kubectl uncordon [node_name]
Remove a node from the cluster:

kubectl delete node [node_name]
Generate a new token:

sudo kubeadm token generate
List the tokens:

sudo kubeadm token list
Print the kubeadm join command to join a node to the cluster:

sudo kubeadm token create [token_name] --ttl 2h --print-join-command


Backing Up and Restoring a Kubernetes Cluster
========================================================


Get the etcd binaries:

wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz
Unzip the compressed binaries:

tar xvf etcd-v3.3.12-linux-amd64.tar.gz
Move the files into /usr/local/bin:

sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin
Take a snapshot of the etcd datastore using etcdctl:

sudo ETCDCTL_API=3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key
View the help page for etcdctl:

ETCDCTL_API=3 etcdctl --help
Browse to the folder that contains the certificate files:

cd /etc/kubernetes/pki/etcd/
View that the snapshot was successful:

ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshot.db
Zip up the contents of the etcd directory:

sudo tar -zcvf etcd.tar.gz etcd
Copy the etcd directory to another server:

scp etcd.tar.gz cloud_user@18.219.235.42:~/


Pod and Node Networking
==========================================


Kubernetes keeps networking simple for effective communication between pods, even if they are located on a different node. In this lesson, we’ll talk about pod communication from within a node, including how to inspect the virtual interfaces, and then get into what happens when a pod wants to talk to another pod on a different node.

See which node our pod is on:

kubectl get pods -o wide
Log in to the node:

ssh [node_name]
View the node's virtual network interfaces:

ifconfig
View the containers in the pod:

docker ps
Get the process ID for the container:

docker inspect --format '{{ .State.Pid }}' [container_id]
Use nsenter to run a command in the process's network namespace:

nsenter -t [container_pid] -n ip addr




When handling traffic from outside sources, there are two ways to direct that traffic to your pods: deploying a load balancer, and creating an ingress controller and an Ingress resource. In this lesson, we will talk about the benefits of each and how Kubernetes distributes traffic to the pods on a node to reduce latency and direct traffic to the appropriate services within your cluster.

View the list of services:

kubectl get services
The load balancer YAML spec:

apiVersion: v1
kind: Service
metadata:
  name: nginx-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: nginx
Create a new deployment:

kubectl run kubeserve2 --image=chadmcrowell/kubeserve2
View the list of deployments:

kubectl get deployments
Scale the deployments to 2 replicas:

kubectl scale deployment/kubeserve2 --replicas=2
View which pods are on which nodes:

kubectl get pods -o wide
Create a load balancer from a deployment:

kubectl expose deployment kubeserve2 --port 80 --target-port 8080 --type LoadBalancer
View the services in your cluster:

kubectl get services
Watch as an external port is created for a service:

kubectl get services -w
Look at the YAML for a service:

kubectl get services kubeserve2 -o yaml
Curl the external IP of the load balancer:

curl http://[external-ip]
View the annotation associated with a service:

kubectl describe services kubeserve
Set the annotation to route load balancer traffic local to the node:

kubectl annotate service kubeserve2 externalTrafficPolicy=Local
The YAML for an Ingress resource:

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: name-virtual-host-ingress
spec:
  rules:
  - host: first.bar.com
    http:
      paths:
      - backend:
          serviceName: service1
          servicePort: 80
  - host: second.foo.com
    http:
      paths:
      - backend:
          serviceName: service2
          servicePort: 80
  - http:
      paths:
      - backend:
          serviceName: service3
          servicePort: 80
Edit the ingress rules:

kubectl edit ingress
View the existing ingress rules:

kubectl describe ingress
Curl the hostname of your Ingress resource:

curl http://kubeserve2.example.com


CoreDNS is now the new default DNS plugin for Kubernetes. In this lesson, we’ll go over the hostnames for pods and services. We will also discover how you can customize DNS to include your own nameservers.

View the CoreDNS pods in the kube-system namespace:

kubectl get pods -n kube-system
View the CoreDNS deployment in your Kubernetes cluster:

kubectl get deployments -n kube-system
View the service that performs load balancing for the DNS server:

kubectl get services -n kube-system
Spec for the busybox pod:

apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox:1.28.4
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
View the resolv.conf file that contains the nameserver and search in DNS:

kubectl exec -it busybox -- cat /etc/resolv.conf
Look up the DNS name for the native Kubernetes service:

kubectl exec -it busybox -- nslookup kubernetes
Look up the DNS names of your pods:

kubectl exec -ti busybox -- nslookup [pod-ip-address].default.pod.cluster.local
Look up a service in your Kubernetes cluster:

kubectl exec -it busybox -- nslookup kube-dns.kube-system.svc.cluster.local
Get the logs of your CoreDNS pods:

kubectl logs [coredns-pod-name]
YAML spec for a headless service:

apiVersion: v1
kind: Service
metadata:
  name: kube-headless
spec:
  clusterIP: None
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubserve2
YAML spec for a custom DNS pod:

apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: dns-example
spec:
  containers:
    - name: test
      image: nginx
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
      - 8.8.8.8
    searches:
      - ns1.svc.cluster.local
      - my.dns.search.suffix
    options:
      - name: ndots
        value: "2"
      - name: edns0


The default scheduler in Kubernetes attempts to find the best node for your pod by going through a series of steps. In this lesson, we will cover the steps in detail in order to better understand the scheduler’s function when placing pods on nodes to maximize uptime for the applications running in your cluster. We will also go through how to create a deployment with node affinity.

Label your node as being located in availability zone 1:

kubectl label node chadcrowell1c.mylabserver.com availability-zone=zone1
Label your node as dedicated infrastructure:

kubectl label node chadcrowell1c.mylabserver.com share-type=dedicated
Here is the YAML for the deployment to include the node affinity rules:

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: pref
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: pref
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: availability-zone
                operator: In
                values:
                - zone1
          - weight: 20
            preference:
              matchExpressions:
              - key: share-type
                operator: In
                values:
                - dedicated
      containers:
      - args:
        - sleep
        - "99999"
        image: busybox
        name: main
Create the deployment:

kubectl create -f pref-deployment.yaml
View the deployment:

kubectl get deployments
View which pods landed on which nodes:

kubectl get pods -o wide


Scheduling pods with Resource limits and label selectors:
==============================================================



In order to share the resources of your node properly, you can set resource limits and requests in Kubernetes. This allows you to reserve enough CPU and memory for your application while still maintaining system health. In this lesson, we will create some requests and limits in our pod YAML to show how it’s used by the node.

View the capacity and the allocatable info from a node:

kubectl describe nodes
The pod YAML for a pod with requests:

apiVersion: v1
kind: Pod
metadata:
  name: resource-pod1
spec:
  nodeSelector:
    kubernetes.io/hostname: "chadcrowell3c.mylabserver.com"
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: pod1
    resources:
      requests:
        cpu: 800m
        memory: 20Mi
Create the requests pod:

kubectl create -f resource-pod1.yaml
View the pods and nodes they landed on:

kubectl get pods -o wide
The YAML for a pod that has a large request:

apiVersion: v1
kind: Pod
metadata:
  name: resource-pod2
spec:
  nodeSelector:
    kubernetes.io/hostname: "chadcrowell3c.mylabserver.com"
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: pod2
    resources:
      requests:
        cpu: 1000m
        memory: 20Mi
Create the pod with 1000 millicore request:

kubectl create -f resource-pod2.yaml
See why the pod with a large request didn’t get scheduled:

kubectl describe resource-pod2
Look at the total requests per node:

kubectl describe nodes chadcrowell3c.mylabserver.com
Delete the first pod to make room for the pod with a large request:

kubectl delete pods resource-pod1
Watch as the first pod is terminated and the second pod is started:

kubectl get pods -o wide -w
The YAML for a pod that has limits:

apiVersion: v1
kind: Pod
metadata:
  name: limited-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      limits:
        cpu: 1
        memory: 20Mi
Create a pod with limits:

kubectl create -f limited-pod.yaml
Use the exec utility to use the top command:

kubectl exec -it limited-pod top
Helpful Links
Configure Default CPU Requests and Limits
Configure Default Memory Requests and Limits





DaemonSets do not use a scheduler to deploy pods. In fact, there are currently DaemonSets in the Kubernetes cluster that we made. In this lesson, I will show you where to find those and how to create your own DaemonSet pods to deploy without the need for a scheduler.

Find the DaemonSet pods that exist in your kubeadm cluster:

kubectl get pods -n kube-system -o wide
Delete a DaemonSet pod and see what happens:

kubectl delete pods [pod_name] -n kube-system
Give the node a label to signify it has SSD:

kubectl label node[node_name] disk=ssd
The YAML for a DaemonSet:

apiVersion: apps/v1beta2
kind: DaemonSet
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: linuxacademycontent/ssd-monitor
Create a DaemonSet from a YAML spec:

kubectl create -f ssd-monitor.yaml
Label another node to specify it has SSD:

kubectl label node chadcrowell2c.mylabserver.com disk=ssd
View the DaemonSet pods that have been deployed:

kubectl get pods -o wide
Remove the label from a node and watch the DaemonSet pod terminate:

kubectl label node chadcrowell3c.mylabserver.com disk-
Change the label on a node to change it to spinning disk:

kubectl label node chadcrowell2c.mylabserver.com disk=hdd --overwrite
Pick the label to choose for your DaemonSet:

kubectl get nodes chadcrowell3c.mylabserver.com --show-labels
Helpful Links
DaemonSets

Displaying scheduler Events
=========================================



There are multiple ways to view the events related to the scheduler. In this lesson, we’ll look at ways in which you can troubleshoot any problems with your scheduler or just find out more information.

View the name of the scheduler pod:

kubectl get pods -n kube-system
Get the information about your scheduler pod events:

kubectl describe pods [scheduler_pod_name] -n kube-system
View the events in your default namespace:

kubectl get events
View the events in your kube-system namespace:

kubectl get events -n kube-system
Delete all the pods in your default namespace:

kubectl delete pods --all
Watch events as they are appearing in real time:

kubectl get events -w
View the logs from the scheduler pod:

kubectl logs [kube_scheduler_pod_name] -n kube-system
The location of a systemd service scheduler pod:

/var/log/kube-scheduler.log


##################################################################


Deploying Application,rolling updates
=============================================



We already know Kubernetes will run pods and deployments, but what happens when you need to update or change the version of your application running inside of the Kubernetes cluster? That’s where rolling updates come in, allowing you to update the app image with zero downtime. In this lesson, we’ll go over a rolling update, how to roll back, and how to pause the update if things aren’t going well.

The YAML for a deployment:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      containers:
      - image: linuxacademycontent/kubeserve:v1
        name: app
Create a deployment with a record (for rollbacks):

kubectl create -f kubeserve-deployment.yaml --record
Check the status of the rollout:

kubectl rollout status deployments kubeserve
View the ReplicaSets in your cluster:

kubectl get replicasets
Scale up your deployment by adding more replicas:

kubectl scale deployment kubeserve --replicas=5
Expose the deployment and provide it a service:

kubectl expose deployment kubeserve --port 80 --target-port 80 --type NodePort
Set the minReadySeconds attribute to your deployment:

kubectl patch deployment kubeserve -p '{"spec": {"minReadySeconds": 10}}'
Use kubectl apply to update a deployment:

kubectl apply -f kubeserve-deployment.yaml
Use kubectl replace to replace an existing deployment:

kubectl replace -f kubeserve-deployment.yaml
Run this curl look while the update happens:

while true; do curl http://10.105.31.119; done
Perform the rolling update:

kubectl set image deployments/kubeserve app=linuxacademycontent/kubeserve:v2 --v 6
Describe a certain ReplicaSet:

kubectl describe replicasets kubeserve-[hash]
Apply the rolling update to version 3 (buggy):

kubectl set image deployment kubeserve app=linuxacademycontent/kubeserve:v3
Undo the rollout and roll back to the previous version:

kubectl rollout undo deployments kubeserve
Look at the rollout history:

kubectl rollout history deployment kubeserve
Roll back to a certain revision:

kubectl rollout undo deployment kubeserve --to-revision=2
Pause the rollout in the middle of a rolling update (canary release):

kubectl rollout pause deployment kubeserve
Resume the rollout after the rolling update looks good:

kubectl rollout resume deployment kubeserve



#########################################################################################

Application HA , configmap and secrets
==========================================



Continuing from the last lesson, we will go through how Kubernetes will save you from EVER releasing code with bugs. Then, we will talk about ConfigMaps and secrets as a way to pass configuration data to your apps.

The YAML for a readiness probe:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubeserve
  minReadySeconds: 10
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      containers:
      - image: linuxacademycontent/kubeserve:v3
        name: app
        readinessProbe:
          periodSeconds: 1
          httpGet:
            path: /
            port: 80
Apply the readiness probe:

kubectl apply -f kubeserve-deployment-readiness.yaml
View the rollout status:

kubectl rollout status deployment kubeserve
Describe deployment:

kubectl describe deployment
Create a ConfigMap with two keys:

kubectl create configmap appconfig --from-literal=key1=value1 --from-literal=key2=value2
Get the YAML back out from the ConfigMap:

kubectl get configmap appconfig -o yaml
The YAML for the ConfigMap pod:

apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
  - name: app-container
    image: busybox:1.28
    command: ['sh', '-c', "echo $(MY_VAR) && sleep 3600"]
    env:
    - name: MY_VAR
      valueFrom:
        configMapKeyRef:
          name: appconfig
          key: key1
Create the pod that is passing the ConfigMap data:

kubectl apply -f configmap-pod.yaml
Get the logs from the pod displaying the value:

kubectl logs configmap-pod
The YAML for a pod that has a ConfigMap volume attached:

apiVersion: v1
kind: Pod
metadata:
  name: configmap-volume-pod
spec:
  containers:
  - name: app-container
    image: busybox
    command: ['sh', '-c', "echo $(MY_VAR) && sleep 3600"]
    volumeMounts:
      - name: configmapvolume
        mountPath: /etc/config
  volumes:
    - name: configmapvolume
      configMap:
        name: appconfig
Create the ConfigMap volume pod:

kubectl apply -f configmap-volume-pod.yaml
Get the keys from the volume on the container:

kubectl exec configmap-volume-pod -- ls /etc/config
Get the values from the volume on the pod:

kubectl exec configmap-volume-pod -- cat /etc/config/key1
The YAML for a secret:

apiVersion: v1
kind: Secret
metadata:
  name: appsecret
stringData:
  cert: value
  key: value
Create the secret:

kubectl apply -f appsecret.yaml
The YAML for a pod that will use the secret:

apiVersion: v1
kind: Pod
metadata:
  name: secret-pod
spec:
  containers:
  - name: app-container
    image: busybox
    command: ['sh', '-c', "echo Hello, Kubernetes! && sleep 3600"]
    env:
    - name: MY_CERT
      valueFrom:
        secretKeyRef:
          name: appsecret
          key: cert
Create the pod that has attached secret data:

kubectl apply -f secret-pod.yaml
Open a shell and echo the environment variable:

kubectl exec -it secret-pod -- sh
echo $MY_CERT
The YAML for a pod that will access the secret from a volume:

apiVersion: v1
kind: Pod
metadata:
  name: secret-volume-pod
spec:
  containers:
  - name: app-container
    image: busybox
    command: ['sh', '-c', "echo $(MY_VAR) && sleep 3600"]
    volumeMounts:
      - name: secretvolume
        mountPath: /etc/certs
  volumes:
    - name: secretvolume
      secret:
        secretName: appsecret
Create the pod with volume attached with secrets:

kubectl apply -f secret-volume-pod.yaml
Get the keys from the volume mounted to the container with the secrets:

kubectl exec secret-volume-pod -- ls /etc/certs




Creating self healing application:
=============================================



In this lesson, we’ll go through the power of ReplicaSets, which make your application self-healing by replicating pods and moving them around and spinning them up when nodes fail. We’ll also talk about StatefulSets and the benefit they provide.

The YAML for a ReplicaSet:

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myreplicaset
  labels:
    app: app
    tier: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: main
        image: linuxacademycontent/kubeserve
Create the ReplicaSet:

kubectl apply -f replicaset.yaml
The YAML for a pod with the same label as a ReplicaSet:

apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    tier: frontend
spec:
  containers:
  - name: main
    image: linuxacademycontent/kubeserve
Create the pod with the same label:

kubectl apply -f pod-replica.yaml
Watch the pod get terminated:

kubectl get pods -w 
The YAML for a StatefulSet:

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
Create the StatefulSet:

kubectl apply -f statefulset.yaml
View all StatefulSets in the cluster:

kubectl get statefulsets
Describe the StatefulSets:

kubectl describe statefulsets


self healing applications
==============================



In this lesson, we’ll go through the power of ReplicaSets, which make your application self-healing by replicating pods and moving them around and spinning them up when nodes fail. We’ll also talk about StatefulSets and the benefit they provide.

The YAML for a ReplicaSet:

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myreplicaset
  labels:
    app: app
    tier: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: main
        image: linuxacademycontent/kubeserve
Create the ReplicaSet:

kubectl apply -f replicaset.yaml
The YAML for a pod with the same label as a ReplicaSet:

apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    tier: frontend
spec:
  containers:
  - name: main
    image: linuxacademycontent/kubeserve
Create the pod with the same label:

kubectl apply -f pod-replica.yaml
Watch the pod get terminated:

kubectl get pods -w 
The YAML for a StatefulSet:

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
Create the StatefulSet:

kubectl apply -f statefulset.yaml
View all StatefulSets in the cluster:

kubectl get statefulsets
Describe the StatefulSets:

kubectl describe statefulsets


Persistance Volumes
============================


In Kubernetes, pods are ephemeral. This creates a unique challenge with attaching storage directly to the filesystem of a container. Persistent Volumes are used to create an abstraction layer between the application and the underlying storage, making it easier for the storage to follow the pods as they are deleted, moved, and created within your Kubernetes cluster.

In the Google Cloud Engine, find the region your cluster is in:

gcloud container clusters list
Using Google Cloud, create a persistent disk in the same region as your cluster:

gcloud compute disks create --size=1GiB --zone=us-central1-a mongodb
The YAML for a pod that will use persistent disk:

apiVersion: v1
kind: Pod
metadata:
  name: mongodb 
spec:
  volumes:
  - name: mongodb-data
    gcePersistentDisk:
      pdName: mongodb
      fsType: ext4
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
Create the pod with disk attached and mounted:

kubectl apply -f mongodb-pod.yaml
See which node the pod landed on:

kubectl get pods -o wide
Connect to the mongodb shell:

kubectl exec -it mongodb mongo
Switch to the mystore database in the mongodb shell:

use mystore
Create a JSON document to insert into the database:

db.foo.insert({name:'foo'})
View the document you just created:

db.foo.find()
Exit from the mongodb shell:

exit
Delete the pod:

kubectl delete pod mongodb
Create a new pod with the same attached disk:

kubectl apply -f mongodb-pod.yaml
Check to see which node the pod landed on:

kubectl get pods -o wide
Drain the node (if the pod is on the same node as before):

kubectl drain [node_name] --ignore-daemonsets
Once the pod is on a different node, access the mongodb shell again:

kubectl exec -it mongodb mongo
Access the mystore database again:

use mystore
Find the document you created from before:

db.foo.find()
The YAML for a PersistentVolume object in Kubernetes:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  capacity: 
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
    - ReadOnlyMany
  persistentVolumeReclaimPolicy: Retain
  gcePersistentDisk:
    pdName: mongodb
    fsType: ext4
Create the Persistent Volume resource:

kubectl apply -f mongodb-persistentvolume.yaml
View our Persistent Volumes:

kubectl get persistentvolumes


Volume Access Modes:
=============================


Volume access modes are how you specify the access of a node to your Persistent Volume. There are three types of access modes: ReadWriteOnce, ReadOnlyMany, and ReadWriteMany. In this lesson, we will explain what each of these access modes means and two VERY IMPORTANT things to remember when using your Persistent Volumes with pods.

The YAML for a Persistent Volume:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  capacity: 
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
    - ReadOnlyMany
  persistentVolumeReclaimPolicy: Retain
  gcePersistentDisk:
    pdName: mongodb
    fsType: ext4
View the Persistent Volumes in your cluster:

kubectl get pv


Persistance Volume claim (PVC)
========================================


Persistent Volume Claims (PVCs) are a way for an application developer to request storage for the application without having to know where the underlying storage is. The claim is then bound to the Persistent Volume (PV), and it will not be released until the PVC is deleted. In this lesson, we will go through creating a PVC and accessing storage within our persistent disk.

The YAML for a PVC:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc 
spec:
  resources:
    requests:
      storage: 1Gi
  accessModes:
  - ReadWriteOnce
  storageClassName: ""
Create a PVC:

kubectl apply -f mongodb-pvc.yaml
View the PVC in the cluster:

kubectl get pvc
View the PV to ensure it’s bound:

kubectl get pv
The YAML for a pod that uses a PVC:

apiVersion: v1
kind: Pod
metadata:
  name: mongodb 
spec:
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
  volumes:
  - name: mongodb-data
    persistentVolumeClaim:
      claimName: mongodb-pvc
Create the pod with the attached storage:

kubectl apply -f mongo-pvc-pod.yaml
Access the mogodb shell:

kubectl exec -it mongodb mongo
Find the JSON document created in previous lessons:

db.foo.find()
Delete the mongodb pod:

kubectl delete pod mogodb
Delete the mongodb-pvc PVC:

kubectl delete pvc mongodb-pvc
Check the status of the PV:

kubectl get pv
The YAML for the PV to show its reclaim policy:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  capacity: 
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
    - ReadOnlyMany
  persistentVolumeReclaimPolicy: Retain
  gcePersistentDisk:
    pdName: mongodb
    fsType: ext4


Storage Objects:
=====================



There’s an even easier way to provision storage in Kubernetes with StorageClass objects. Also, your storage is safe from data loss with the “Storage Object in Use Protection” feature, which ensures any pods using a Persistent Volume will not lose the data on the volume as long as it is actively mounted. We’ve been using Google Storage for this section, but there are many different volume types you can use in Kubernetes. In this lesson, we will talk about the hostPath volume and the empty directory volume type.

See the PV protection on your volume:

kubectl describe pv mongodb-pv
See the PVC protection for your claim:

kubectl describe pvc mongodb-pvc
Delete the PVC:

kubectl delete pvc mongodb-pvc
See that the PVC is terminated, but the volume is still attached to pod:

kubectl get pvc
Try to access the data, even though we just deleted the PVC:

kubectl exec -it mongodb mongo
use mystore
db.foo.find()
Delete the pod, which finally deletes the PVC:

kubectl delete pods mongodb
Show that the PVC is deleted:

kubectl get pvc
YAML for a StorageClass object:

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
Create the StorageClass type "fast":

kubectl apply -f sc-fast.yaml
Change the PVC to include the new StorageClass object:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc 
spec:
  storageClassName: fast
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce
Create the PVC with automatically provisioned storage:

kubectl apply -f mongodb-pvc.yaml
View the PVC with new StorageClass:

kubectl get pvc
View the newly provisioned storage:

kubectl get pv
The YAML for a hostPath PV:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
spec:
  storageClassName: local-storage
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
The YAML for a pod with an empty directory volume:

apiVersion: v1
kind: Pod
metadata:
  name: emptydir-pod
spec:
  containers:
  - image: busybox
    name: busybox
    command: ["/bin/sh", "-c", "while true; do sleep 3600; done"]
    volumeMounts:
    - mountPath: /tmp/storage
      name: vol
  volumes:
  - name: vol
    emptyDir: {}



Application with persistance storage:
=======================================================

In this lesson, we’ll wrap everything up in a nice little bow and create a deployment that will allow us to use our storage with our pods. This is to demonstrate how a real-world application would be deployed and used for storing data.

The YAML for our StorageClass object:

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
The YAML for our PVC:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kubeserve-pvc 
spec:
  storageClassName: fast
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce
Create our StorageClass object:

kubectl apply -f storageclass-fast.yaml
View the StorageClass objects in your cluster:

kubectl get sc
Create our PVC:

kubectl apply -f kubeserve-pvc.yaml
View the PVC created in our cluster:

kubectl get pvc
View our automatically provisioned PV:

kubectl get pv
The YAML for our deployment:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      containers:
      - env:
        - name: app
          value: "1"
        image: linuxacademycontent/kubeserve:v1
        name: app
        volumeMounts:
        - mountPath: /data
          name: volume-data
      volumes:
      - name: volume-data
        persistentVolumeClaim:
          claimName: kubeserve-pvc
Create our deployment and attach the storage to the pods:

kubectl apply -f kubeserve-deployment.yaml
Check the status of the rollout:

kubectl rollout status deployments kubeserve
Check the pods have been created:

kubectl get pods
Connect to our pod and create a file on the PV:

kubectl exec -it [pod-name] -- touch /data/file1.txt
Connect to our pod and list the contents of the /data directory:

kubectl exec -it [pod-name] -- ls /data


Lab:
Creating Persistent Storage for Pods in Kubernetes
=======================================================


In this hands-on lab, to decouple our storage from our pods, we will create a persistent volume to mount for use by our pods. We will deploy a mongodb image that will contain a MongoDB database. We will first create the persistent volume, then create the pod YAML for deploying the pod to mount the volume. We will then delete the pod and create a new pod, which will access that same volume.

Log in to the Kube Master server using the credentials on the lab page (either in your local terminal, using the Instant Terminal feature, or using the public IP), and work through the objectives listed.

Create a PersistentVolume.
Use the following YAML spec for the PersistentVolume named mongodb-pv.yaml:

 apiVersion: v1
 kind: PersistentVolume
 metadata:
   name: mongodb-pv
 spec:
   storageClassName: local-storage
   capacity:
     storage: 1Gi
   accessModes:
     - ReadWriteOnce
   hostPath:
     path: "/mnt/data"


Then, create the PersistentVolume:

 kubectl apply -f mongodb-pv.yaml
Create a PersistentVolumeClaim.
Use the following YAML spec for the PersistentVolumeClaim named mongodb-pvc.yaml:

 apiVersion: v1
 kind: PersistentVolumeClaim
 metadata:
   name: mongodb-pvc
 spec:
   storageClassName: local-storage
   accessModes:
     - ReadWriteOnce
   resources:
     requests:
       storage: 1Gi
Then, create the PersistentVolumeClaim:

 kubectl apply -f mongodb-pvc.yaml
Create a pod from the mongodb image, with a mounted volume to mount path /data/db.
Use the following YAML spec for the pod named mongodb-pod.yaml:

 apiVersion: v1
 kind: Pod
 metadata:
   name: mongodb
 spec:
   containers:
   - image: mongo
     name: mongodb
     volumeMounts:
     - name: mongodb-data
       mountPath: /data/db
     ports:
     - containerPort: 27017
       protocol: TCP
   volumes:
   - name: mongodb-data
     persistentVolumeClaim:
       claimName: mongodb-pvc
Then, create the pod:

 kubectl apply -f mongodb-pod.yaml
Verify the pod was created:

 kubectl get pods
Access the node and view the data within the volume.
Run the following command:

 kubectl get nodes
Connect to the worker node (get the <node_hostname> from the NAME column of the above output), using the same password as the Kube Master:

 ssh <node_hostname>
Switch to the /mnt/data directory:

 cd /mnt/data
List the contents of the directory:

 ls
Delete the pod and create a new pod with the same YAML spec.
Exit out of the worker node:

 exit
Delete the pod:

 kubectl delete pod mongodb
Create a new pod:

 kubectl apply -f mongodb-pod.yaml
Verify the data still resides on the volume.
Log in to the worker node again:

 ssh <node_hostname>
Switch to the /mnt/data directory:

 cd /mnt/data
List the contents of the directory:

 ls

###########Securing Kubernetes Cluster:
==========================================


Configuring Network Policies:
===================================

Network policies allow you to specify which pods can talk to other pods. This helps when securing communication between pods, allowing you to identify ingress and egress rules. You can apply a network policy to a pod by using pod or namespace selectors. You can even choose a CIDR block range to apply the network policy. In this lesson, we’ll go through each of these options for network policies.

Download the canal plugin:

wget -O canal.yaml https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/canal/canal.yaml
Apply the canal plugin:

kubectl apply -f canal.yaml
The YAML for a deny-all NetworkPolicy:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
Run a deployment to test the NetworkPolicy:

kubectl run nginx --image=nginx --replicas=2
Create a service for the deployment:

kubectl expose deployment nginx --port=80
Attempt to access the service by using a busybox interactive pod:

kubectl run busybox --rm -it --image=busybox /bin/sh
#wget --spider --timeout=1 nginx



The YAML for a pod selector NetworkPolicy:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-netpolicy
spec:
  podSelector:
    matchLabels:
      app: db
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: web
    ports:
    - port: 5432
Label a pod to get the NetworkPolicy:

kubectl label pods [pod_name] app=db
The YAML for a namespace NetworkPolicy:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ns-netpolicy
spec:
  podSelector:
    matchLabels:
      app: db
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          tenant: web
    ports:
    - port: 5432




The YAML for an IP block NetworkPolicy:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ipblock-netpolicy
spec:
  podSelector:
    matchLabels:
      app: db
  ingress:
  - from:
    - ipBlock:
        cidr: 192.168.1.0/24



The YAML for an egress NetworkPolicy:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: egress-netpol
spec:
  podSelector:
    matchLabels:
      app: web
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: db
    ports:
    - port: 5432


Creating TLS Certificates
==================================


A Certificate Authority (CA) is used to generate TLS certificates and authenticate to your API server. In this lesson, we’ll go through certificate requests and generating a new certificate.

Find the CA certificate on a pod in your cluster:

kubectl exec busybox -- ls /var/run/secrets/kubernetes.io/serviceaccount
Download the binaries for the cfssl tool:

wget -q --show-progress --https-only --timestamping \
  https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \
  https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
Make the binary files executable:

chmod +x cfssl_linux-amd64 cfssljson_linux-amd64
Move the files into your bin directory:

sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl
sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson
Check to see if you have cfssl installed correctly:

cfssl version
Create a CSR file:

cat <<EOF | cfssl genkey - | cfssljson -bare server
{
  "hosts": [
    "my-svc.my-namespace.svc.cluster.local",
    "my-pod.my-namespace.pod.cluster.local",
    "172.168.0.24",
    "10.0.34.2"
  ],
  "CN": "my-pod.my-namespace.pod.cluster.local",
  "key": {
    "algo": "ecdsa",
    "size": 256
  }
}
EOF
Create a CertificateSigningRequest API object:

cat <<EOF | kubectl create -f -
apiVersion: certificates.k8s.io/v1beta1
kind: CertificateSigningRequest
metadata:
  name: pod-csr.web
spec:
  groups:
  - system:authenticated
  request: $(cat server.csr | base64 | tr -d '\n')
  usages:
  - digital signature
  - key encipherment
  - server auth
EOF
View the CSRs in the cluster:

kubectl get csr
View additional details about the CSR:

kubectl describe csr pod-csr.web
Approve the CSR:

kubectl certificate approve pod-csr.web
View the certificate within your CSR:

kubectl get csr pod-csr.web -o yaml
Extract and decode your certificate to use in a file:

kubectl get csr pod-csr.web -o jsonpath='{.status.certificate}' \
    | base64 --decode > server.crt




Securing Persistent Key Value Store:
========================================

Secrets are used to secure sensitive data you may access from your pod. The data never gets written to disk because it's stored in an in-memory filesystem (tmpfs). Because secrets can be created independently of pods, there is less risk of the secret being exposed during the pod lifecycle.

View the secrets in your cluster:

kubectl get secrets
View the default secret mounted to each pod:

kubectl describe pods pod-with-defaults
View the token, certificate, and namespace within the secret:

kubectl describe secret
Generate a key for your https server:

openssl genrsa -out https.key 2048
Generate a certificate for the https server:

openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj /CN=www.example.com
Create an empty file to create the secret:

touch file
Create a secret from your key, cert, and file:

kubectl create secret generic example-https --from-file=https.key --from-file=https.cert --from-file=file
View the YAML from your new secret:

kubectl get secrets example-https -o yaml
Create the configMap that will mount to your pod:

apiVersion: v1
kind: ConfigMap
metadata:
  name: config
data:
  my-nginx-config.conf: |
    server {
        listen              80;
        listen              443 ssl;
        server_name         www.example.com;
        ssl_certificate     certs/https.cert;
        ssl_certificate_key certs/https.key;
        ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;
        ssl_ciphers         HIGH:!aNULL:!MD5;

        location / {
            root   /usr/share/nginx/html;
            index  index.html index.htm;
        }

    }
  sleep-interval: |
    25
The YAML for a pod using the new secret:

apiVersion: v1
kind: Pod
metadata:
  name: example-https
spec:
  containers:
  - image: linuxacademycontent/fortune
    name: html-web
    env:
    - name: INTERVAL
      valueFrom:
        configMapKeyRef:
          name: config
          key: sleep-interval
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    - name: config
      mountPath: /etc/nginx/conf.d
      readOnly: true
    - name: certs
      mountPath: /etc/nginx/certs/
      readOnly: true
    ports:
    - containerPort: 80
    - containerPort: 443
  volumes:
  - name: html
    emptyDir: {}
  - name: config
    configMap:
      name: config
      items:
      - key: my-nginx-config.conf
        path: https.conf
  - name: certs
    secret:
      secretName: example-https
Describe the nginx conf via ConfigMap:

kubectl describe configmap
View the cert mounted on the container:

kubectl exec example-https -c web-server -- mount | grep certs
Use port forwarding on the pod to server traffic from 443:

kubectl port-forward example-https 8443:443 &
Curl the web server to get a response:

curl https://localhost:8443 -k


Creating a ClusterRole to Access a PV in Kubernetes
======================================================


We have been given access to a three-node cluster. Within that cluster, a PV has already been provisioned. We will need to make sure we can access the PV directly from a pod in our cluster. By default, pods cannot access PVs directly, so we will need to create a ClusterRole and test the access after it's been created. Every ClusterRole requires a ClusterRoleBinding to bind the role to a user, service account, or group. After we have created the ClusterRole and ClusterRoleBinding, we will try to access the PV directly from a pod.

Log in to the Kube Master server using the credentials on the lab page (either in your local terminal, using the Instant Terminal feature, or using the public IP), and work through the objectives listed.

View the Persistent Volume.
Use the following command to view the Persistent Volume within the cluster:

 kubectl get pv
Create a ClusterRole.
Use the following command to create the ClusterRole:

 kubectl create clusterrole pv-reader --verb=get,list --resource=persistentvolumes
Create a ClusterRoleBinding.
Use the following command to create the ClusterRoleBinding:

 kubectl create clusterrolebinding pv-test --clusterrole=pv-reader --serviceaccount=web:default
Create a pod to access the PV.
Use the following YAML to create a pod that will proxy the connection and allow you to curl the address:

 apiVersion: v1
 kind: Pod
 metadata:
   name: curlpod
   namespace: web
 spec:
   containers:
   - image: tutum/curl
     command: ["sleep", "9999999"]
     name: main
   - image: linuxacademycontent/kubectl-proxy
     name: proxy
   restartPolicy: Always
Use the following command to create the pod:

 kubectl apply -f curlpod.yaml
Request access to the PV from the pod.
Use the following command (from within the pod) to access a shell from the pod:

 kubectl exec -it curlpod -n web -- sh
Use the following command to curl the PV resource:

 curl localhost:8001/api/v1/persistentvolumes


ouput:

cloud_user@ip-10-0-1-101:~$ kubectl exec -it curlpod -n web -- sh
Defaulting container name to main.
Use 'kubectl describe pod/curlpod -n web' to see all of the containers in this pod.
# curl localhost:8001/api/v1/persistentvolumes
{
  "kind": "PersistentVolumeList",
  "apiVersion": "v1",
  "metadata": {
    "selfLink": "/api/v1/persistentvolumes",
    "resourceVersion": "3790"
  },
  "items": [
    {
      "metadata": {
        "name": "database-pv",
        "selfLink": "/api/v1/persistentvolumes/database-pv",
        "uid": "5abc04c4-84b7-11e9-b024-0a249169e49c",
        "resourceVersion": "491",
        "creationTimestamp": "2019-06-01T21:51:04Z",
        "annotations": {
          "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolume\",\"metadata\":{\"annotations\":{},\"name\":\"database-pv\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"capacity\":{\"storage\":\"1Gi\"},\"hostPath\":{\"path\":\"/mnt/data\"},\"storageClassName\":\"local-storage\"}}\n"
        },
        "finalizers": [
          "kubernetes.io/pv-protection"
        ]
      },
      "spec": {
        "capacity": {
          "storage": "1Gi"
        },
        "hostPath": {
          "path": "/mnt/data",
          "type": ""
        },
        "accessModes": [
          "ReadWriteOnce"
        ],
        "persistentVolumeReclaimPolicy": "Retain",
        "storageClassName": "local-storage",
        "volumeMode": "Filesystem"
      },
      "status": {
        "phase": "Available"
      }
    }
  ]
}#     




Monitoring the Cluster Components
=============================================================


We are able to monitor the CPU and memory utilization of our pods and nodes by using the metrics server. In this lesson, we’ll install the metrics server and see how the kubectl top command works.

Clone the metrics server repository:

git clone https://github.com/linuxacademy/metrics-server
Install the metrics server in your cluster:

kubectl apply -f ~/metrics-server/deploy/1.8+/
Get a response from the metrics server API:

kubectl get --raw /apis/metrics.k8s.io/
Get the CPU and memory utilization of the nodes in your cluster:

kubectl top node
Get the CPU and memory utilization of the pods in your cluster:

kubectl top pods
Get the CPU and memory of pods in all namespaces:

kubectl top pods --all-namespaces
Get the CPU and memory of pods in only one namespace:

kubectl top pods -n kube-system
Get the CPU and memory of pods with a label selector:

kubectl top pod -l run=pod-with-defaults
Get the CPU and memory of a specific pod:

kubectl top pod pod-with-defaults
Get the CPU and memory of the containers inside the pod:

kubectl top pods group-context --containers


Liveliness and rediness Probe:
====================================


There are ways Kubernetes can automatically monitor your apps for you and, furthermore, fix them by either restarting or preventing them from affecting the rest of your service. You can insert liveness probes and readiness probes to do just this for custom monitoring of your applications.

The pod YAML for a liveness probe:

apiVersion: v1
kind: Pod
metadata:
  name: liveness
spec:
  containers:
  - image: linuxacademycontent/kubeserve
    name: kubeserve
    livenessProbe:
      httpGet:
        path: /
        port: 80



The YAML for a service and two pods with readiness probes:

apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: nginx
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    readinessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 5
---
apiVersion: v1
kind: Pod
metadata:
  name: nginxpd
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:191
    readinessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 5
Create the service and two pods with readiness probes:

kubectl apply -f readiness.yaml
Check if the readiness check passed or failed:

kubectl get pods
Check if the failed pod has been added to the list of endpoints:

kubectl get ep
Edit the pod to fix the problem and enter it back into the service:

kubectl edit pod [pod_name]
Get the list of endpoints to see that the repaired pod is part of the service again:

kubectl get ep


Managing Cluster Components logs:
====================================

There are many ways to manage the logs that can accumulate from both applications and system components. In this lesson, we’ll go through a few different approaches to organizing your logs.

The directory where the continainer logs reside:

/var/log/containers
The directory where kubelet stores its logs:

/var/log
The YAML for a pod that has two different log streams:

apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}
Create a pod that has two different log streams to the same directory:

kubectl apply -f twolog.yaml
View the logs in the /var/log directory of the container:

kubectl exec counter -- ls /var/log
The YAML for a sidecar container that will tail the logs for each type:

apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log-1
    image: busybox
    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log-2
    image: busybox
    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}
View the first type of logs separately:

kubectl logs counter count-log-1
View the second type of logs separately:

kubectl logs counter count-log-2


Managing Application Logs
============================

Containerized applications usually write their logs to standard out and standard error instead of writing their logs to files. Docker then redirects those streams to files. You can retrieve those files with the kubectl logs command in Kubernetes. In this lesson, we’ll go over the many ways to manipulate the output of your logs and redirect them to a file.

Get the logs from a pod:

kubectl logs nginx
Get the logs from a specific container on a pod:

kubectl logs counter -c count-log-1
Get the logs from all containers on the pod:

kubectl logs counter --all-containers=true
Get the logs from containers with a certain label:

kubectl logs -lapp=nginx
Get the logs from a previously terminated container within a pod:

kubectl logs -p -c nginx nginx
Stream the logs from a container in a pod:

kubectl logs -f -c count-log-1 counter
Tail the logs to only view a certain number of lines:

kubectl logs --tail=20 nginx
View the logs from a previous time duration:

kubectl logs --since=1h nginx
View the logs from a container within a pod within a deployment:

kubectl logs deployment/nginx -c nginx
Redirect the output of the logs to a file:

kubectl logs counter -c count-log-1 > count.log






Troubleshooting:
=============================


Troubleshooting Application Failure:
========================================

Application failure can happen for many reasons, but there are ways within Kubernetes that make it a little easier to discover why. In this lesson, we’ll fix some broken pods and show common methods to troubleshoot.

The YAML for a pod with a termination reason:

apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
  - image: busybox
    name: main
    command:
    - sh
    - -c
    - 'echo "I''ve had enough" > /var/termination-reason ; exit 1'
    terminationMessagePath: /var/termination-reason
One of the first steps in troubleshooting is usually to describe the pod:

kubectl describe po pod2
The YAML for a liveness probe that checks for pod health:

apiVersion: v1
kind: Pod
metadata:
  name: liveness
spec:
  containers:
  - image: linuxacademycontent/kubeserve
    name: kubeserve
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8081
View the logs for additional detail:

kubectl logs pod-with-defaults
Export the YAML of a running pod, in the case that you are unable to edit it directly:

kubectl get po pod-with-defaults -o yaml --export > defaults-pod.yaml
Edit a pod directly (i.e., changing the image):

kubectl edit po nginx


Troubleshooting Control Plane Failure:
=============================================

The Kubernetes Control Plane is an important component to back up and protect against failure. There are certain best practices you can take to ensure you don’t have a single point of failure. If your Control Plane components are not effectively communicating, there are a few things you can check to ensure your cluster is operating efficiently.

Check the events in the kube-system namespace for errors:

kubectl get events -n kube-system
Get the logs from the individual pods in your kube-system namespace and check for errors:

kubectl logs [kube_scheduler_pod_name] -n kube-system
Check the status of the Docker service:

sudo systemctl status docker
Start up and enable the Docker service, so it starts upon bootup:

sudo systemctl enable docker && systemctl start docker
Check the status of the kubelet service:

sudo systemctl status kubelet
Start up and enable the kubelet service, so it starts up when the machine is rebooted:

sudo systemctl enable kubelet && systemctl start kubelet
Turn off swap on your machine:

sudo su -
swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab
Check if you have a firewall running:

sudo systemctl status firewalld
Disable the firewall and stop the firewalld service:

sudo systemctl disable firewalld && systemctl stop firewalld



Troubleshooting Worker Node Failure:
=============================================


Troubleshooting worker node failure is a lot like troubleshooting a non-responsive server, in addition to the kubectl tools we have at our disposal. In this lesson, we’ll learn how to recover a node and add it back to the cluster and find out how to identify when the kublet service is down.

Listing the status of the nodes should be the first step:

kubectl get nodes
Find out more information about the nodes with kubectl describe:

kubectl describe nodes chadcrowell2c.mylabserver.com
You can try to log in to your server via SSH:

ssh chadcrowell2c.mylabserver.com
Get the IP address of your nodes:

kubectl get nodes -o wide
Use the IP address to further probe the server:

ssh cloud_user@172.31.29.182
Generate a new token after spinning up a new server:

sudo kubeadm token generate
Create the kubeadm join command for your new worker node:

sudo kubeadm token create [token_name] --ttl 2h --print-join-command
View the journalctl logs:

sudo journalctl -u kubelet
View the syslogs:

sudo more syslog | tail -120 | grep kubelet



Troubleshooting Networking:
=============================


Network issues usually start to arise internally or when using a service. In this lesson, we’ll go through the many methods to see if your app is serving traffic by creating a service and testing the communication within the cluster.

Run a deployment using the container port 9376 and with three replicas:

kubectl run hostnames --image=k8s.gcr.io/serve_hostname \
                        --labels=app=hostnames \
                        --port=9376 \
                        --replicas=3
List the services in your cluster:

kubectl get svc
Create a service by exposing a port on the deployment:

kubectl expose deployment hostnames --port=80 --target-port=9376
Run an interactive busybox pod:

kubectl run -it --rm --restart=Never busybox --image=busybox:1.28 sh
From the pod, check if DNS is resolving hostnames:

# nslookup hostnames
From the pod, cat out the /etc/resolv.conf file:

# cat /etc/resolv.conf
From the pod, look up the DNS name of the Kubernetes service:

# nslookup kubernetes.default
Get the JSON output of your service:

kubectl get svc hostnames -o json
View the endpoints for your service:

kubectl get ep
Communicate with the pod directly (without the service):

wget -qO- 10.244.1.6:9376
Check if kube-proxy is running on the nodes:

ps auxw | grep kube-proxy
Check if kube-proxy is writing iptables:

iptables-save | grep hostnames
View the list of kube-system pods:

kubectl get pods -n kube-system
Connect to your kube-proxy pod in the kube-system namespace:

kubectl exec -it kube-proxy-cqptg -n kube-system -- sh
Delete the flannel CNI plugin:

kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml
Apply the Weave Net CNI plugin:

kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"





KUBERNETES THE HARDWAY:
=================================


Provisioning of CA Certificate:
=====================================


In order to generate the certificates needed by Kubernetes, you must first provision a certificate authority. This lesson will guide you through the process of provisioning a new certificate authority for your Kubernetes cluster. After completing this lesson, you should have a certificate authority, which consists of two files: ca-key.pem and ca.pem.

Here are the commands used in the demo:

cd ~/
mkdir kthw
cd kthw/
UPDATE: cfssljson and cfssl will need to be installed. To install, complete the following commands:

sudo curl -s -L -o /bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
sudo curl -s -L -o /bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
sudo curl -s -L -o /bin/cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
sudo chmod +x /bin/cfssl*
Use this command to generate the certificate authority. Include the opening and closing curly braces to run this entire block as a single command.

{

cat > ca-config.json << EOF
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "8760h"
      }
    }
  }
}
EOF

cat > ca-csr.json << EOF
{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "CA",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert -initca ca-csr.json | cfssljson -bare ca

}



Generate Client certificate:

1) Admin client certificate

2) Kubelet client certificate (  this will be for each worker server)

3) Generate for kube controller mgr

4)kubbe proxy

5) kube scheduler


example:

Now that you have provisioned a certificate authority for the Kubernetes cluster, you are ready to begin generating certificates. The first set of certificates you will need to generate consists of the client certificates used by various Kubernetes components. In this lesson, we will generate the following client certificates: admin, kubelet (one for each worker node), kube-controller-manager, kube-proxy, and kube-scheduler. After completing this lesson, you will have the client certificate files which you will need later to set up the cluster.

Here are the commands used in the demo. The command blocks surrounded by curly braces can be entered as a single command:

cd ~/kthw
Admin Client certificate:

{

cat > admin-csr.json << EOF
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:masters",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin

}
Kubelet Client certificates. Be sure to enter your actual cloud server values for all four of the variables at the top:

WORKER0_HOST=<Public hostname of your first worker node cloud server>
WORKER0_IP=<Private IP of your first worker node cloud server>
WORKER1_HOST=<Public hostname of your second worker node cloud server>
WORKER1_IP=<Private IP of your second worker node cloud server>

{
cat > ${WORKER0_HOST}-csr.json << EOF
{
  "CN": "system:node:${WORKER0_HOST}",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:nodes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${WORKER0_IP},${WORKER0_HOST} \
  -profile=kubernetes \
  ${WORKER0_HOST}-csr.json | cfssljson -bare ${WORKER0_HOST}

cat > ${WORKER1_HOST}-csr.json << EOF
{
  "CN": "system:node:${WORKER1_HOST}",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:nodes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${WORKER1_IP},${WORKER1_HOST} \
  -profile=kubernetes \
  ${WORKER1_HOST}-csr.json | cfssljson -bare ${WORKER1_HOST}

}


Generating the Kubernetes API Server Certificate

HANDS ON:
=============


Building a Kubernetes Cluster with Kubeadm
Introduction
A Kubernetes cluster is a powerful tool for managing containers in a highly-available manner. Kubeadm greatly simplifies the process of setting up a simple cluster. In this hands-on lab, you will build your own working Kubernetes cluster using Kubeadm.

Solution
Begin by logging in to the lab servers using the credentials provided on the hands-on lab page:

Install Docker on all three nodes
====================================



ssh cloud_user@PUBLIC_IP_ADDRESS
Install Docker on all three nodes
Do the following on all three nodes:

Add the Docker GPG key:

 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
Add the Docker repository:

 sudo add-apt-repository \
    "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
    $(lsb_release -cs) \
    stable"
Update packages:

 sudo apt-get update
Install Docker:

 sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu
Hold Docker at this specific version:

 sudo apt-mark hold docker-ce
Verify that Docker is up and running with:

 sudo systemctl status docker
Make sure the Docker service status is active (running)!

Install Kubeadm, Kubelet, and Kubectl on all three nodes
==================================================================
Install the Kubernetes components by running this on all three nodes:

Add the Kubernetes GPG key:

 curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
Add the Kubernetes repository:

 cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
 deb https://apt.kubernetes.io/ kubernetes-xenial main
 EOF
Update packages:

 sudo apt-get update
Install kubelet, kubeadm, and kubectl:

 sudo apt-get install -y kubelet=1.12.7-00 kubeadm=1.12.7-00 kubectl=1.12.7-00
Hold the Kubernetes components at this specific version:

 sudo apt-mark hold kubelet kubeadm kubectl


Bootstrap the cluster on the Kube master node
=============================================================

On the Kube master node, do this:

 sudo kubeadm init --pod-network-cidr=10.244.0.0/16
That command may take a few minutes to complete.

When it is done, set up the local kubeconfig:

 mkdir -p $HOME/.kube
 sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
 sudo chown $(id -u):$(id -g) $HOME/.kube/config
Take note that the kubeadm init command printed a long kubeadm join command to the screen. You will need that kubeadm join command in the next step!

Run the following command on the Kube master node to verify it is up and running:

 kubectl version
This command should return both a Client Version and a Server Version.

Join the two Kube worker nodes to the cluster
=============================================================


Copy the kubeadm join command that was printed by the kubeadm init command earlier, with the token and hash. Run this command on both worker nodes, but make sure you add sudo in front of it:

 sudo kubeadm join $some_ip:6443 --token $some_token --discovery-token-ca-cert-hash $some_hash
Now, on the Kube master node, make sure your nodes joined the cluster successfully:

 kubectl get nodes
Verify that all three of your nodes are listed. It will look something like this:

 NAME            STATUS     ROLES    AGE   VERSION
 ip-10-0-1-101   NotReady   master   30s   v1.12.2
 ip-10-0-1-102   NotReady   <none>   8s    v1.12.2
 ip-10-0-1-103   NotReady   <none>   5s    v1.12.2
Note that the nodes are expected to be in the NotReady state for now.

Set up cluster networking with flannel
=======================================================


Turn on iptables bridge calls on all three nodes:

 echo "net.bridge.bridge-nf-call-iptables=1" | sudo tee -a /etc/sysctl.conf
 sudo sysctl -p
Next, run this only on the Kube master node:

 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml
Now flannel is installed! Make sure it is working by checking the node status again:

 kubectl get nodes
After a short time, all three nodes should be in the Ready state. If they are not all Ready the first time you run kubectl get nodes, wait a few moments and try again. It should look something like this:

 NAME            STATUS   ROLES    AGE   VERSION
 ip-10-0-1-101   Ready    master   85s   v1.12.2
 ip-10-0-1-102   Ready    <none>   63s   v1.12.2
 ip-10-0-1-103   Ready    <none>   60s   v1.12.2


##################################################################


Deploying a Simple Service to Kubernetes
Introduction
Deployments and services are at the core of what makes Kubernetes a great way to manage complex application infrastructures. In this hands-on lab, you will have an opportunity to get hands-on with a Kubernetes cluster and build a simple deployment, coupled with a service providing access to it. You will create a deployment and a service which can be accessed by other pods in the cluster.

Solution
Begin by logging in to the Kubernetes Master server using the credentials provided on the hands-on lab page:

 ssh cloud_user@PUBLIC_IP_ADDRESS
Create a deployment for the store-products service with four replicas
Log in to the Kube master node.
Create the deployment with four replicas:

 cat << EOF | kubectl apply -f -
 apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: store-products
     labels:
       app: store-products
   spec:
     replicas: 4
     selector:
       matchLabels:
         app: store-products
     template:
       metadata:
         labels:
           app: store-products
       spec:
         containers:
         - name: store-products
           image: linuxacademycontent/store-products:1.0.0
           ports:
           - containerPort: 80
 EOF
Create a store-products service and verify that you can access it from the busybox testing pod
Create a service for the store-products pods:

 cat << EOF | kubectl apply -f -
    kind: Service
    apiVersion: v1
    metadata:
      name: store-products
    spec:
      selector:
        app: store-products
      ports:
      - protocol: TCP
        port: 80
        targetPort: 80
 EOF
Make sure the service is up in the cluster:

 kubectl get svc store-products
The output will look something like this:

 NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
 store-products   ClusterIP   10.104.11.230   <none>        80/TCP    59s
Use kubectl exec to query the store-products service from the busybox testing pod.

 kubectl exec busybox -- curl -s store-products


Taints and tollerations:
====================================


Learning Objectives
check_circle
Taint one of the worker nodes to repel work.
keyboard_arrow_up
Use the following command to taint the node:

kubectl taint node <node_name> node-type=prod:NoSchedule
check_circle
Schedule a pod to the dev environment.
keyboard_arrow_up
Use the following YAML to specify a pod that will be scheduled to the dev environment:

apiVersion: v1
kind: Pod
metadata:
 name: dev-pod
 labels:
   app: busybox
spec:
 containers:
 - name: dev
   image: busybox
   command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 3600']
Use the following command to create the pod:

kubectl create -f dev-pod.yaml
check_circle
Allow a pod to be scheduled to the prod environment.
keyboard_arrow_up
Use the following YAML to create a deployment and a pod that will tolerate the prod environment:

apiVersion: apps/v1
kind: Deployment
metadata:
 name: prod
spec:
 replicas: 1
 selector:
   matchLabels:
     app: prod
 template:
   metadata:
     labels:
       app: prod
   spec:
     containers:
     - args:
       - sleep
       - "3600"
       image: busybox
       name: main
     tolerations:
     - key: node-type
       operator: Equal
       value: prod
       effect: NoSchedule
Use the following command to create the pod:

kubectl create -f prod-deployment.yaml
check_circle
Verify each pod has been scheduled and verify the toleration.
keyboard_arrow_up
Use the following command to verify the pods have been scheduled:

kubectl get pods -o wide
Verify the toleration of the production pod:

kubectl get pods <pod_name> -o yaml





Repairing the pods
====================================



Repairing Failed Pods in Kubernetes
In this hands-on lab, we will be presented with a number of broken pods. We must identify the problem and take the quickest route to resolve the problem in order to get our cluster back up and running.

Log in to the Kube Master server using the credentials on the lab page (either in your local terminal, using the Instant Terminal feature, or using the public IP), and work through the objectives listed.

Identify the broken pods.
Use the following command to see what’s in the cluster:

 kubectl get all --all-namespaces
To make this a little easier to read, you could run the following command to view services, pods, and deployments:

 kubectl get svc,po,deploy -n web
Find out why the pods are broken.
Use the following command to inspect one of the broken pods and view the events:

 kubectl describe pod <pod_name>
Repair the broken pods.
Use the following command to repair the broken pods in the most efficient manner:

 kubectl edit deploy nginx -n web
Where it says image: nginx:191, change it to image: nginx. Save and exit.

Ensure pod health by accessing the pod directly.
Use the following command to access the pod directly via its container port:

 wget -qO- <pod_ip_address>:8080


#################################################################################



Upgrading the Kubernetes Cluster Using kubeadm
==================================================

We have been given a three-node cluster that is in need of an upgrade. In this hands-on lab, we must perform the upgrade to all of the cluster components, including kubeadm, kube-controller-manager, kube-scheduler, kubeadm, and kubectl.

Log in to all three servers using the credentials on the lab page (either in your local terminal, using the Instant Terminal feature, or using the public IPs), and work through the objectives listed.

Get the latest version of kubeadm.
In the terminal where you're logged in to the Master node, use the following commands to create a variable and get the latest version of kubeadm:

export VERSION=$(curl -sSL https://dl.k8s.io/release/stable.txt)
export ARCH=amd64
curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubeadm > kubeadm
Install kubeadm and verify it has been installed correctly.
Still in the Master node terminal, run the following commands to install kubeadm and verify the version:

sudo install -o root -g root -m 0755 ./kubeadm /usr/bin/kubeadm
sudo kubeadm version
Plan the upgrade in order to check for errors.
Still in the Master node terminal, use the following command to plan the upgrade:

sudo kubeadm upgrade plan
Perform the upgrade of the kube-scheduler and kube-controller-manager.
Still in the Master node terminal, use this command to apply the upgrade (also in the output of upgrade plan):

sudo kubeadm upgrade apply v1.13.3
Get the latest version of kubelet.
Now, in each node terminal, use the following commands to get the latest version of kubelet on each node:

export VERSION=$(curl -sSL https://dl.k8s.io/release/stable.txt)
export ARCH=amd64
curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubelet > kubelet
Install kubelet on each node and restart the kubelet service.
In each node terminal, use these commands to install kubelet and restart the kubelet service:

sudo install -o root -g root -m 0755 ./kubelet /usr/bin/kubelet
sudo systemctl restart kubelet.service
Verify the kubelet was installed correctly.
Use the following command to verify the kubelet was installed correctly:

kubectl get nodes
Get the latest version of kubectl.
In each node terminal, use the following command to get the latest version of kubectl:

curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubectl > kubectl
Install the latest version of kubectl.
In each node terminal, use the following command to install the latest version of kubectl:

sudo install -o root -g root -m 0755 ./kubectl /usr/bin/kubectl



#####################################################################################################


Bootstrapping Kubernetes Worker Nodes
Introduction
In this lab, we'll get hands-on experience setting up new Kubernetes worker nodes for a cluster.

Your team wants to set up a new Kubernetes cluster. Control nodes have already been created and are ready to be used. However, no worker nodes have been set up. You have been given the task of setting up two worker nodes to be used as part of the new Kubernetes cluster.

To start, open two terminal windows and log in to both worker servers using the public IPs and credentials listed on the lab page. Note: We're going to run all the commands on both servers simultaneously throughout the entire lab.

Install the Required Packages
Install the packages on both worker nodes:

sudo apt-get -y install socat conntrack ipset
Download and Install the Necessary Binaries
You can download and install the binaries like this:

wget -q --show-progress --https-only --timestamping \
  https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \
  https://storage.googleapis.com/kubernetes-the-hard-way/runsc \
  https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \
  https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz \
  https://github.com/containerd/containerd/releases/download/v1.1.0/containerd-1.1.0.linux-amd64.tar.gz \
  https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet

sudo mkdir -p \
  /etc/cni/net.d \
  /opt/cni/bin \
  /var/lib/kubelet \
  /var/lib/kube-proxy \
  /var/lib/kubernetes \
  /var/run/kubernetes

chmod +x kubectl kube-proxy kubelet runc.amd64 runsc

sudo mv runc.amd64 runc

sudo mv kubectl kube-proxy kubelet runc runsc /usr/local/bin/

sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/

sudo tar -xvf cni-plugins-amd64-v0.6.0.tgz -C /opt/cni/bin/

sudo tar -xvf containerd-1.1.0.linux-amd64.tar.gz -C /
Configure the containerd Service
Configure the containerd service like this:

sudo mkdir -p /etc/containerd/
Create the containerd config.toml.

cat << EOF | sudo tee /etc/containerd/config.toml
[plugins]
  [plugins.cri.containerd]
    snapshotter = "overlayfs"
    [plugins.cri.containerd.default_runtime]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runc"
      runtime_root = ""
    [plugins.cri.containerd.untrusted_workload_runtime]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runsc"
      runtime_root = "/run/containerd/runsc"
EOF
Create the containerd unit file:

cat << EOF | sudo tee /etc/systemd/system/containerd.service
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target

[Service]
ExecStartPre=/sbin/modprobe overlay
ExecStart=/bin/containerd
Restart=always
RestartSec=5
Delegate=yes
KillMode=process
OOMScoreAdjust=-999
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity

[Install]
WantedBy=multi-user.target
EOF
Configure the kubelet Service
You can set up kubelet like this. Make sure you set HOSTNAME to worker0 on the first worker node and worker1 on the second.

HOSTNAME=<worker1 or worker0, depending on the server>;.mylabserver.com

sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/

sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig

sudo mv ca.pem /var/lib/kubernetes/
Create the kubelet config file:

cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/${HOSTNAME}.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/${HOSTNAME}-key.pem"
EOF
Create the kubelet unit file:

cat << EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --network-plugin=cni \\
  --register-node=true \\
  --v=2 \\
  --hostname-override=${HOSTNAME} \\
  --allow-privileged=true
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
Configure the kube-proxy Service
You can configure kube-proxy like this:

sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
Create the kube-proxy config file:

cat << EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "10.200.0.0/16"
EOF
Create the kube-proxy unit file:

cat << EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

Successfully Start All of the Services


Enable and start all of the services like this:

sudo systemctl daemon-reload

sudo systemctl enable containerd kubelet kube-proxy

sudo systemctl start containerd kubelet kube-proxy
You can verify that the services are up and running like this:

sudo systemctl status containerd kubelet kube-proxy
Make sure containerd, kubelet, and kube-proxy are all in the active (running) state on both worker nodes.

Now, make sure both nodes are registering with the cluster. Log in to the control node and run this command:

kubectl get nodes --kubeconfig /home/cloud_user/admin.kubeconfig
Make sure your two worker nodes appear. Note they will likely not be in the READY state. For now, just make sure they both show up.



Prometheus Monitoring:
==============================


./bootstrap.sh

kubectl apply -f clusterRole.yml
kubectl apply -f namespaces.yml
kubectl apply -f prometheus-config-map.yml
kubectl apply -f prometheus-deployment.yml
kubectl apply -f kube-state-metrics.yml




vi namespaces.yaml

{
  "kind": "Namespace",
  "apiVersion": "v1",
  "metadata": {
    "name": "monitoring",
    "labels": {
      "name": "monitoring"
    }
  }
}

vi ClusterRole.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: default
  namespace: monitoring


vi prometheus-config-map.yml

apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-server-conf
  labels:
    name: prometheus-server-conf
  namespace: monitoring
data:
  prometheus.yml: |-
    global:
      scrape_interval: 5s
      evaluation_interval: 5s

vi prometheus-deployment.yml

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: prometheus-deployment
  namespace: monitoring
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: prometheus-server
    spec:
      containers:
        - name: prometheus
          image: prom/prometheus:v2.2.1
          args:
            - "--config.file=/etc/prometheus/prometheus.yml"
            - "--storage.tsdb.path=/prometheus/"
            - "--web.enable-lifecycle"
          ports:
            - containerPort: 9090
          volumeMounts:
            - name: prometheus-config-volume
              mountPath: /etc/prometheus/
            - name: prometheus-storage-volume
              mountPath: /prometheus/
      volumes:
        - name: prometheus-config-volume
          configMap:
            defaultMode: 420
            name: prometheus-server-conf

        - name: prometheus-storage-volume
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus-service
  namespace: monitoring
  annotations:
      prometheus.io/scrape: 'true'
      prometheus.io/port:   '9090'

spec:
  selector:
    app: prometheus-server
  type: NodePort
  ports:
    - port: 8080
      targetPort: 9090
      nodePort: 8080



vi kube-state-metrics.yml

apiVersion: v1
kind: Service
metadata:
  name: kube-state-metrics
  namespace: monitoring
  labels:
    app: kube-state-metrics
  annotations:
    prometheus.io/scrape: 'true'
spec:
  ports:
  - name: metrics
    port: 8080
    targetPort: metrics
    protocol: TCP
  selector:
    app: kube-state-metrics
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: monitoring
  labels:
    app: kube-state-metrics
spec:
  replicas: 1
  template:
    metadata:
      name: kube-state-metrics-main
      labels:
        app: kube-state-metrics
    spec:
      containers:
        - name: kube-state-metrics
          image: quay.io/coreos/kube-state-metrics:latest
          ports:
          - containerPort: 8080
            name: metrics



